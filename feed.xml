<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:8080/feed.xml" rel="self" type="application/atom+xml"/><link href="http://localhost:8080/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-24T06:58:42+00:00</updated><id>http://localhost:8080/feed.xml</id><title type="html">eigentaylor</title><subtitle>i do math sometimes </subtitle><entry><title type="html">Jordan Canonical Form Made Easier</title><link href="http://localhost:8080/blog/2025/jordan2/" rel="alternate" type="text/html" title="Jordan Canonical Form Made Easier"/><published>2025-06-21T00:00:00+00:00</published><updated>2025-06-21T00:00:00+00:00</updated><id>http://localhost:8080/blog/2025/jordan2</id><content type="html" xml:base="http://localhost:8080/blog/2025/jordan2/"><![CDATA[<p>Okay, so Jordan Canonical Form is a very strange topic. It’s usually relegated to advanced linear algebra (which it is), but it’s also presented in a very strange way. You see, JCF is actually part of a much deeper result found in Abstract Algebra! Specifically: Ring Theory about finitely generated modules over a PID (principal ideal domain). That’s a lot of words!</p> <p>The idea is that finitely generated Modules over a PID have a very predictable and clear structure. And we can actually identify every linear operator on some vector space \(V\) (over the field \(F\)) with a finitely generated torsion module \(V\) over the ring \(F[x]\) (all polynomials with coefficients in \(F\) with variable \(x\)). And after a lot of incredibly difficult abstract algebra, you can prove the existence of a Jordan Canonical form!</p> <p>Is the abstract algebra actually better than the seemingly random theorems and proofs used to prove the existence of JCF without abstract algebra? Well… I think so. But the point I <em>really</em> want to get across to you is the following:</p> <ol> <li>Proving the existence of the Jordan Canonical Form is HARD and unintuitive</li> <li>Using the JCF to find a Jordan basis isn’t nearly as bad</li> </ol> <p>Now, you’re probably thinking, “Taylor, can you just explain Jordan forms to me?” And to that I say, “yeah, shortly”. But the idea I really want to get across is that the <em>reason</em> Jordan forms are so darn confusing and sound like the ravings of a lunatic is <em>because</em> it’s actually an extension of a much deeper result in Abstract Algebra. And, I think, with that context, things just come out of nowhere and there’s no rhyme or reason to it. So my plan is to set the groundwork by briefly explaining some of the abstract algebra, effectively offloading the existence of the Jordan form, so that we can focus on using our knowledge of <a href="../changeofbasis/" target="_blank">change of basis</a> to actually <em>find</em> the Jordan form.</p> <h2 id="the-abstract-algebra">The Abstract Algebra</h2> <p>Basically, linear algebra is done on vector spaces which are over a <em>field</em>. A field is stuff that can be added and multiplied, and all the nonzero stuff can be divided out (like the real numbers or complex numbers). A ring is like a field except you can’t always divide stuff (and multiplication isn’t always commutative). Rings are a little weirder (and more interesting in my opinion!).</p> <p>An ideal of a commutative ring is kind of like a subspace for a ring: closed under addition (by other things in the ideal) and multiplication (by anything else in the ring). The even numbers are an example of an ideal of the ring of integer. And a PID is just a commutative ring where every ideal is generated by one element (like every even number is a multiple of 2). For our purposes, we just need to understand the PID \(F[x]\).</p> <p>\(F[x]\) is the ring of single variable polynomials (in \(x\)) with coefficients in the field \(F\). This is, in fact, a PID! The proof is not really relevant, but it comes down to the existence of a division algorithm for polynomials.</p> <p>You can do linear algebra over a ring instead of a field, and then we call it a module. Without all the nice field properties, modules can be SUPER weird. For example, some modules can have linearly independent spanning (generating) sets of ANY size. That is, the same module has a linearly independent generating set of size 1 and also of size 2901457!. But I’m getting too off topic (point is, you should study modules if you like linear algebra!).</p> <p>We’re specifically going to talk about the structure of finitely generated <em>torsion</em> modules over \(F[x]\). What does that mean? Well, basically, we’re going to assume that we have an operator \(T\) on \(V\) (finite dimensional). Then the module \(V\) over \(F[x]\) is basically kind of like a vector space where we have vectors \(v\) but our scalars are in \(F[x]\). And we define “scalar multiplication” as</p> \[\begin{equation} p(x)\cdot v=p(T)v \end{equation}\] <p>We are going to assume \(T\) has a minimal polynomial (we’ll call \(m(x)\). note \(m\) is nonzero), then \(m(T)=0\), so \(m(x)\cdot v=0\) for all \(v\). Remember that the minimal polynomial is just the polynomial of least degree such that \(m(T)=0\). If it sounds weird that <em>any</em> polynomial with that property should exist, the Cayley Hamilton theorem tells us that the characteristic polynomial does have that property, in fact! The minimal polynomial is always going to be something that divides the characteristic polynomial. Anyway, since we have a nonzero scalar (polynomial \(m\)) that multiplies ALL vectors to \(0\), we say this is a “torsion module”.</p> <h3 id="the-structure-theorem">The Structure Theorem</h3> <p>We’re just going to write the structure theorem for our specific case of \(V\) as a \(F[x]\) torsion module. Since \(V\) is finite dimensional (meaning it has a finite basis), then it’s finitely generated. This means the module can be decomposed as</p> \[V\cong F[x]/(p_1(x))\oplus\ldots\oplus F[x]/(p_g(x))\] <p>Meaning \(V\) can be decomposed as being isomorphic to separate cyclic submodules \(F[x]/(p_i(x))\), but what are they? Well, for one thing, we know they must divide the minimal polynomial. But it turns out, one can show that they will be decomposed into</p> \[V\cong \bigoplus F[x]/(x-\lambda_i)^{k_i}\] <p>where \(\lambda_1,\ldots,\lambda_s\) are the eigenvalues of \(T\). And it turns out each of these submodules \(F[x]/(x-\lambda_i)^{k_i}\) correspond to Jordan blocks. More specifically, these make up the generalized eigenspaces.</p> <p>Why is this intuitive? Well, it isn’t, really. But I can try my best to give you a sense.</p> <p>Eigenspaces are invariant and independent. It’s elementary to show that eigenvectors with different eigenvalues are linearly independent. So the idea that we can decompose \(V\) into a direct sum of eigenspaces isn’t entirely radical. This is what we do when \(T\) is diagonalizable. And it turns out, the diagonalizable case is when all the \(k_i\)’s are \(1\). It’s only when \(V\) gets decomposed into a term like \(F[x]/(x-\lambda)^2\) (we call this a non-simple submodule) that it becomes nondiagonalizable.</p> <p>Okay, but why? What the heck is \(F[x]/(q(x))\) anyway? Basically, it’s the set of polynomials of the form</p> \[r(x)+(q(x))\] <p>where we define</p> <p>And what does it mean to have a subspace corresponding to \(F[x]/(x-\lambda)^2\) or \(F[x]/(x-\lambda)^k\) (\(k&gt;1\))? It means that all the vectors in that space are annihilated by \((x-\lambda)^k\). That is:</p> \[(T-\lambda I)^kv=0\] <p>for all \(v\) in the submodule. But the submodule is cyclic. So we have some generator \(v_k\).</p> <p>This gives us a new way to think about nondiagonalizability.</p> \[\begin{equation} \begin{pmatrix}1\\1\end{pmatrix} \end{equation}\] <p><a href="../eigentricks/" target="_blank">hyperlink</a></p> <p><a href="#c2">Section c2</a></p> <p><a href="https://youtu.be/g2VYkc-MtC8?si=7KvPWXyo4wPEhZ7f" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[not easy. just like... easier...]]></summary></entry><entry><title type="html">Why do we row reduce? What IS a matrix?</title><link href="http://localhost:8080/blog/2024/rref/" rel="alternate" type="text/html" title="Why do we row reduce? What IS a matrix?"/><published>2024-07-27T00:00:00+00:00</published><updated>2024-07-27T00:00:00+00:00</updated><id>http://localhost:8080/blog/2024/rref</id><content type="html" xml:base="http://localhost:8080/blog/2024/rref/"><![CDATA[<p>I’ve stood by and let this go on for too long. As someone who has been tutoring/teaching linear algebra for five years now, I’ve looked at the subject as a whole through multiple different perspectives. We have this abstract difficult subject with a million terms and concepts (many terms actually being the SAME concept under a different name), problems that can look entirely different but end up asking the exact same question, AND most students have to figure it out while they are learning how to properly do a mathematical proof for the first time (which is NOT an easy skill to learn). No wonder people struggle with it! And, so, I can understand why many professors choose to offload the concepts and underlying “algebra” until later in the course, and start off with “Here’s a system of equations, row reduce it!” for three weeks. And though tedious, and generally unmotivated, it’s certainly… easy?</p> <p>The problem is that with so much to cover, my experience with most students is that the key concepts that tie EVERYTHING together are not being emphasized. So when the more important topics like basis, linear independence, span, coordinate vectors, kernel, image, etc. are introduced, the pathways between the new topic and what the student has already learned aren’t being drawn, and it all seems so disconnected. More like you’re learning something completely different solved with a similar method, rather than learning an extension of a previous concept.</p> <p>So that’s what I want to do today. I’m going to start from square one, and we’re going to change how we view row reduction, matrix multiplication, and even a matrix <em>itself</em>. And with this new perspective, I hope that everything feels more cohesive and straightforward.</p> <h3 id="image-terminology">Image Terminology</h3> <p>This post is pretty much entirely about functions, and to <em>really</em> talk about functions properly, we’re going to use the proper terminology. These are concepts you’re probably familiar with, but perhaps under different names.</p> <p>We denote \(f:X\to Y\) to mean \(f\) is a function that takes stuff in the <strong>domain</strong> \(X\) to the <strong>codomain</strong> \(Y\). i.e. <em>everything</em> in \(X\) <em>can</em> be put into the function, and every output is in \(Y\) (though we can’t necessarily reach everything in \(Y\)). For example, \(\exp:\mathbb{R}\to\mathbb{R}\) for \(\exp(x)=e^x\) is a function that takes in real numbers and outputs real numbers (so the \(\mathbb{R}\) is both the domain and codomain). This is because we can put in any real number, and we’ll get out a real number, so we write . Even though every output of \(e^x\) is actually strictly positive (there’s no intput to obtain \(-1\), for example), it’s still correct to say it’s a function with codomain \(\mathbb{R}\).</p> <p>If \(f(x)=y\), then we say \(y\) is <em>the</em> <strong>image</strong> of \(x\). Images are <em>unique</em> (because a function must be “well-defined”). If you vaguely remember that a function can’t send one input to two outputs, or that it must pass the “vertical line test”, this is basically the more rigorous way to say it: the image is unique.</p> <p>It’s also very useful to say that \(f(x)=y\) means that \(x\) is <em>a</em> <strong>preimage</strong> of \(y\). Preimages are not necessarily unique. For example, with the function \(f(x)=x^2\), \(f(2)=f(-2)=4\), so \(4\) has two preimages: \(2,-2\).</p> <p>We call the complete set of all possible outputs that \(f\) can produce the <strong>image/range of \(f\)</strong>. And for special types of functions (like the ones we study in linear algebra), we call the set of all preimages of \(0\) the <strong>kernel</strong>.</p> <p>This is important to talk about up front, because understanding why matrices are useful requires understanding how the images of a few special vectors under special functions allows us to simplify how we <em>write</em> that function.</p> <h2 id="what-is-a-matrix">What IS a matrix</h2> <p>Now, some smart-asses (many of which, I love) might object to what I’m about to present and say, “a matrix is JUST an array of numbers which CAN induce a linear transformation!” And, yeah, but we’re talking in the context of linear algebra, here. In my opinion, the way to view a matrix that provides the most intuitive bang for your buck is as a <strong>Linear Transformation</strong> itself.</p> <h3 id="linear-transformations">Linear Transformations</h3> <p>First, though, I’m going to give you the completely oversimplified low-down of what the majority of abstract algebra is like, on the motivational level.</p> <blockquote> <p>“We have some things that interact in some nice way, let’s look at functions and substructures that preserve those interactions.”</p> </blockquote> <p>In terms of linear algebra, our “things” are vectors (over some field of scalars), and the interaction is vector addition and scalar multiplication.</p> <p><strong>NOTE: You can think of a “field” as just some collection of “numbers” that you can add/subtract/multiply and divide (if they’re nonzero). Just think \(\mathbb{R}\) or \(\mathbb{C}\) for the purposes of this post.</strong></p> <p>We can do these things and still get a vector, and combining these operations is called taking a “linear combination”.</p> \[v=c_1v_1+\ldots+c_nv_n\] <p>So, in a sense, we can view vector spaces as some collection of vectors, and we can take linear combinations of those vectors using scalars from the field the vector space is over.</p> <p>NOTE: Saying scalars from a “field” is important! There are other nonvector things we can use to “scale/multiply to” vectors, but being able to divide nonzero scalars is what makes linear algebra so nice (and what makes Modules–vector spaces over a ring instead of a field–much crazier).</p> <p>So, algebraists, then, are interested in functions and substructures of the vector space that preserve the interactions of vectors (the interactions are linear combinations, in our case). The substructures of vector spaces that preserve linear combinations are <strong>subspaces</strong>, and the functions that preserve linear combinations are called <strong>Linear Transformations</strong>. Another sort of “type” of substructure is a <strong>quotient space</strong> (but that’s far beyond the scope of this post). Our focus today is on linear transformations, today.</p> <p>For a linear transformation \(T\) to preserve linear combinations, it means that for any \(v=c_1v_1+\ldots+c_nv_n\),</p> \[\begin{gather*} T(v)=T(c_1v_1+\ldots+c_nv_n)\\=c_1T(v_1)+\ldots+c_nT(v_n) \end{gather*}\] <p>That is, the image of a linear combination is the linear combination of the images.</p> <p>But why are <em>you</em> (someone who is probably <em>not</em> an algebraist) interested in functions that preserve linear combinations? Well, if you’re taken calculus, then you definitely are! How do you take the derivative of a polynomial?</p> \[\frac{d}{dx}(ax^2+bx+c)=a\frac{d}{dx}x^2+b\frac{d}{dx}x+c\frac{d}{dx}1\] \[=a(2x)+b(1)+c(0)=2ax+b\] <p>You probably don’t think about it that way (with so many steps), and you can just <em>see</em> the derivative. But this is really what’s going on under the hood. We don’t have an explicit formula for the derivative of every single polynomial that exists, but we do know</p> <ul> <li>The derivative of any single power of \(x\): \(\frac{d}{dx}x^n=nx^{n-1}\).</li> <li>We can just take constants “along for the ride”. That is, we can pull scalars out of a derivative.</li> <li>We can <em>also</em> separate the derivative of a sum into the sum of derivatives.</li> </ul> \[\begin{gather*} \frac{d}{dx}(cf(x))=c\frac{d}{dx}f(x)\\ \frac{d}{dx}(f(x)+g(x))=\frac{d}{dx}f(x)+\frac{d}{dx}g(x)\\ \end{gather*}\] <p>We can combine these properties to just say the derivative “preserves” linear combinations:</p> \[\frac{d}{dx}(c_1f(x)+c_2g(x))=c_1\frac{d}{dx}f(x)+c_2\frac{d}{dx}g(x)\] <p>and so differentiating a polynomial is just breaking up the linear combination and applying the derivative to each power of \(x\). This is the key idea: we only need to know how the operation acts on each vector, and then we know exactly how it acts on any linear combination of those vectors. We don’t need to compute it from scratch.</p> <p>And, in that way, the derivative is one of the most important examples of a linear transformation (<a href="../linalglinconstcoef/" target="_blank">and the study of the derivative as a linear operator on smooth functions is pretty much the most important part of an Ordinary Differential Equations course</a>). The integral can also be considered a linear operator.</p> <p>Hopefully, the derivative and integral are sufficient to convince you that “yeah, studying linear operators might be worth doing”, but in case you need a few more, the following are examples of things that are actually just applications of linear transformations:</p> <ul> <li>Systems of linear equations</li> <li>State changes (Markov chains, PageRank algorithms)</li> <li>Adjacency matrices for a graph</li> <li>Fourier and Laplace transforms</li> </ul> <h2 id="representing-a-linear-transformation">Representing a linear transformation</h2> <p>Alright, so I’ll assume you’re on board with “linear transformations are worth studying”. Now, we’re going to restrict ourselves to linear transformations between finite dimensional vector spaces (the main topic for an introductory course in linear algebra). For our purposes, a finite dimensional vector space is necessary because it has a finite basis, which is the key fact that makes matrices so useful.</p> <p>Now, what do I mean by finite basis? Well, let’s focus on the main dish of finite dimensional vector spaces: coordinate spaces! You might know them in the case of \(F=\mathbb{R}\) as “Euclidean vector spaces” of the form \(\mathbb{R}^n\). We’ll just call them \(F^n\): an array or list of \(n\) numbers from the underlying field of scalars (which we’re just calling \(F\) instead of specifying \(\mathbb{R}\) or \(\mathbb{C}\) or whatever other field have).</p> <p>Note: One of the reasons linear algebra is so powerful is that all the non \(F^n\) finite dimensional vector spaces are actually just isomorphic to \(F^n\) by the coordinate map. In layman’s terms: it’s all just \(F^n\). We can without loss of generality assume we’re taking about \(F^n\) coordinate spaces, which mesh the best with matrices. And then we only have to slightly adjust the execution for general finite dimensional vector spaces.</p> <h3 id="basis">Basis</h3> <p>Okay, so why do we care about a “basis”, and what is it? A <strong>basis</strong> is a “generating set” of vectors with unique representations. What does that mean?</p> <h4 id="span-and-generating-sets">Span and generating sets</h4> <p>The fact that it’s a “generating set” means that the basis can “generate” or produce any vector in the space. If a vector exists in the space, I can give you a linear combination of the basis vectors that equals that vector. If our basis is \(\left\{v_1,\ldots,v_n\right\}\), then that means for all \(x\in F^n\), we can find constants \(c_1,\ldots, c_n\) depending on \(x\) such that</p> \[x=c_1v_1+\ldots+c_nv_n\] <p>You probably learned the word “span” to describe this concept, but “generate” is a more descriptive word (in my opinion), <em>and</em> the more general term used for similar concepts in abstract algebra. But it’s still the same thing: we’ll use “generates” as a synonym for “spans”.</p> <p>For example, take \(F^2\) with the standard basis \(\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}\). For any vector in the space, \(x=\begin{pmatrix}x_1\\x_2\end{pmatrix}\), we can easily find a linear combination of the basis vectors to reach \(x\):</p> \[x=\begin{pmatrix}x_1\\x_2\end{pmatrix}=x_1\begin{pmatrix}1\\0\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}\] <p>The constants in the linear combination are just the entries! That’s what makes this the “standard basis”. It’s the simplest possible basis, which is easiest to use. We’ll use \(e_i\) for the \(i\)th standard basis vector (that is, a \(1\) in the \(i\)th entry and zeroes everywhere else).</p> <p>I would like to remark about the connection between span/generation and the substructure I mentioned earlier: “subspaces”. A subspace is a subcollection of vectors which are <strong>closed</strong> under linear combinations. Meaning any linear combinations of vectors in the subspace is still in the subspace. It turns out that subspaces are exactly just spans of some generating set. That is, every subspace is the span of some set of vectors in the subspace, and every span of a set of vectors is itself a subspace. But it’s better to think of span and subspace being synonymous ideas, rather than a happenstance. Our primary interaction of vectors is linear combinations, and span is just taking <em>all</em> linear combinations. Can you justify why that would necessarily form a closed structure under linear combinations? The other direction is actually almost silly: any structure closed under linear combinations (subspace) is just the span of itself. You <em>can</em> say something less trivial by using the axiom of choice to justify that any vector space (and any subspace is itself a vector space) has a basis (and, thus, a generating set).</p> <p>Later I mention the row space and column space of a matrix. These are examples of subspaces which are defined by being generated by the rows or columns respectively. Note that the kernel is also a subspace.</p> <h4 id="linear-independence">Linear independence</h4> <p>Now, there’s one other part from the definition for basis I gave above: the generating set has “unique representation”, which essentially means that it’s “linearly independent”. This means that the representation of a vector as a linear combination of the basis vectors is unique. So if we have</p> \[x=c_1v_1+\ldots+c_nv_n=d_1v_1+\ldots+d_nv_n\] <p>Then that implies that \(c_i=d_i\) for all \(i\).</p> <p>Now, if you’ve already learned linear independence, you might be thinking “WTF THIS IS NOT WHAT I LEARNED LINEAR INDEPENDENCE MEANS”. And, yes, this is not the standard definition. But it is the most <em>conceptually useful</em> definition, especially for our purposes here (though not the easiest to use for proofs). See, if we stick with just this idea of “unique representations”, then what if \(x=0\)? We can clearly use \(d_1=\ldots=d_n=0\) as a linear combination to get \(x\) because \(0v_1+\ldots+0v_n=0\). However, this means that, by our established uniqueness,</p> \[c_1v_1+\ldots+c_nv_n=0\implies c_1=\ldots=c_n=0\] <p>All the \(c_i\)’s must be zero! And <em>this</em> is now closer to what you probably learned before. In fact, you can actually read the more familiar definition as saying “the zero vector can only (uniquely) be represented as a linear combination of these vectors using the trivial linear combination of all zero coefficients (trivial solution)”. And, as it turns out, if \(0\) can only be represented by the trivial solution, then that means ALL representations are unique! This is not as obvious (and worth trying to prove if you can)!</p> <p>The other super important and useful fact: <strong>all bases have the same size!</strong> Don’t take this for granted, the fact we’re over a field is the only reason this is guaranteed to be true. There are Modules which can have a generating sets of any positive integer size which are <em>all</em> linearly independent. For us, it’s SUPER simple: <strong>\(F^n\) has \(n\) basis vectors</strong>. Don’t overthink it!</p> <p>Okay, so basically I took your familiar “basis” = “spans” + “linearly independent” definition, and slapped on confusing words: “spans” = “generates” and “linearly independent” = “unique representations”. Why? Because now everything is going to come together.</p> <h3 id="all-together-now">All together now</h3> <p>Okay, so to summarize so far:</p> <ul> <li>We have vector spaces over some field which contain vectors that we can take linear combinations of.</li> <li>We’re interested in functions called Linear Transformations that preserve linear combinations.</li> <li>If the vector space is finite dimensional, then we have some finite basis.</li> <li>A basis gives a way to uniquely write any vector in the space.</li> </ul> <p>We’re going to use these facts together to get the main reason linear transformations are so nice and easy to study: <strong>To know how it acts on ANY vector, we ONLY need to know how it acts on the BASIS vectors of the domain.</strong></p> <p>This is <strong>HUGE</strong>. Functions can be very complicated and complex! It can be computationally intense to even compute many functions. But with linear transformations between finite dimensional vector spaces? Easy-peasy. Because if \(T\) is a linear transformation from \(F^n\to F^m\), then we only need to know the images of the \(n\) basis vectors of \(F^n\) to know where \(T\) sends <strong>EVERY</strong> vector. (I feel like you aren’t getting as excited about this as I am)</p> <p>So suppose you are trying to tell your friend across the sea about your interesting linear transformation between \(\mathbb{R}^3\to \mathbb{R}^2\). How do you do it? \(\mathbb{R}^3\) has infinitely many vectors (uncountably many, in fact). You could never write down every image (output) for every vector in \(\mathbb{R}^3\). But, you don’t need to! For,</p> \[\begin{multline*} T\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=T(x_1e_1+x_2e_2+x_3e_3)\\=x_1T(e_1)+x_2T(e_2)+x_3T(e_3) \end{multline*}\] <p>Therefore, all we need is \(T(e_1),T(e_2),T(e_3)\). That’s just three \(\mathbb{R}^2\) vectors: which is \(6\) simple numbers total. That means, all you need to send to your friend is those six numbers, and they will have EXACTLY your linear transformation. Let’s say the images are</p> \[T(e_1)=\begin{pmatrix}1\\1\end{pmatrix},\quad T(e_2)=\begin{pmatrix}0\\1\end{pmatrix},\quad T(e_3)=\begin{pmatrix}1\\-1\end{pmatrix}\] <p>So, if you were lazy, and you wanted to send those six numbers to your friend in as clear and simple a way as possible, how might you do it?</p> <p>Well sending just <code class="language-plaintext highlighter-rouge">1,1,0,1,1,-1</code> is kind of confusing and hard to parse. It’s also not immediately clear what this is a transformation between. It could be \(\mathbb{R}^1\) to \(\mathbb{R}^6\) or vice versa, or \(\mathbb{R}^2\) to \(\mathbb{R}^3\). So a one-dimensional array doesn’t really work.</p> <p>Okay, you know exactly what I’m getting at: YOU PUT IT IN A <strong>MATRIX</strong>. A <strong>TWO</strong>-dimensional array. And we have the vectors already in their little column form, why not just concatenate them? Let’s define</p> \[A=\begin{pmatrix}1&amp;0&amp;1\\1&amp;1&amp;-1\end{pmatrix}\] <p>to be our little package that <em>encodes</em> \(T\). This perfectly stores all the information we need to communicate \(T\). Each column tell us the image of each standard basis vector.</p> <p>Now, let’s take it one step further. What if we could use \(A\) <em>instead</em> of \(T\)? What if we didn’t have to write the linear transformation at all? What if we could stick to just these simple arrays of numbers to carry <strong>all</strong> the information of our linear transformation? How would we do it?</p> <p>Well we clearly want \(Ax=T(x)\). That much is obvious. And we know that</p> \[T\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=x_1\begin{pmatrix}1\\1\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}+x_3\begin{pmatrix}1\\-1\end{pmatrix}\] <p>So… then… let’s just define</p> \[\begin{multline*} \begin{pmatrix}1&amp;0&amp;1\\1&amp;1&amp;-1\end{pmatrix}\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}\\ =x_1\begin{pmatrix}1\\1\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}+x_3\begin{pmatrix}1\\-1\end{pmatrix} \end{multline*}\] <p>And there, now we have matrix vector multiplication defined.</p> <p>Essentially, all we’re doing is</p> \[\begin{equation} A= \Bigg(\begin{matrix}T(e_1)&amp;\cdots&amp;T(e_n)\end{matrix}\Bigg) \end{equation}\] <p><img src="/assets/img/all coming together.png" alt="all coming together" style="width:100%; max-width:600px;"/></p> <p>Observe that this also, at a glance, tells us exactly what \(T\) is a function between. Each column has two entries (i.e. \(A\) has two rows), so the codomain must be \(\mathbb{R}^2\). We also have three columns, so our domain has three basis vectors. Clearly, the domain must be \(\mathbb{R}^3\). Thus, our construction of \(A\) as a “package” for \(T\) means that \(A\) being \(m\times n\) (\(m\) rows and \(n\) columns) means that the linear transformation that \(A\) encodes is from \(F^n\to F^m\).</p> <p>I’d like to acknowledge that with this perspective, a student doesn’t have to “memorize” the fact that \(m\times n\iff F^n\to F^m\). This isn’t a “theorem” or something that requires excessive scratch work. This is a damn <strong>feature</strong> of matrices, which makes them so informative and useful. Not only do the columns encode the outputs, the very <strong>shape</strong> itself tells you one of the most important aspects of the function: the dimension of the domain and codomain (AND their size relative to each other).</p> <p>So by our construction, this allows us to utilize other theorems to know from a glance that</p> <ul> <li>A wide matrix has a nontrivial kernel (is not injective)</li> <li>A tall matrix is not surjective</li> </ul> <h3 id="change-of-basis">Change of basis</h3> <p>Now, it’s time for my trump card. Why you should <em>definitely</em> view a matrix from the perspective that the columns are the images of the basis vectors. Because that’s how we define a matrix for a linear transformation with respect to <em>other</em> bases! If your course covers finding a matrix with respect to a certain basis, then you <em>absolutely</em> should start with this perspective.</p> <p>I see it, time and time again. A student asks how to find the matrix for \(T\) with respect to some given basis \(\beta\) of the domain and \(\gamma\) for the codomain (or maybe just the matrix for a described linear transformation), and they’re lost. And the inevitable first question someone asks the student is “do you know the ‘formula’ for the matrix with respect to the bases \(\beta\) and \(\gamma\)?”</p> \[[T]_\beta^\gamma = \Bigg(\begin{matrix}\left[T(\beta_1)\right]_\gamma&amp;\cdots&amp;\left[T(\beta_n)\right]_\gamma\end{matrix}\Bigg)\] <p>But, really, this is the <strong>definition</strong> for the matrix representation for a given linear transformation when bases are specified for the domain and codomain. Now, if you adopt the view I’m trying to push here, then this definition is just… the natural generalization. The columns are still just the images of the basis vectors of the domain, we’re just being more explicit that we’re using a specific basis. But now we make sure the images are the coordinates with respect to the proper basis of the codomain. That is literally the only change. If both \(\beta\) and \(\gamma\) are just the standard basis of \(F^n\) and \(F^m\) respectively, it’s <em>exactly</em> what we described above.</p> <p>If you haven’t learned coordinate vectors or change of basis, you can ignore this. But just know that this is <em>literally</em> how we define the matrix representation of a linear transformation!</p> <p><a href="https://youtu.be/t348e24vDyA?si=k3Df2elcbvqxdcnq" target="_blank">So why isn’t this view emphasized at the beginning of a course so that this generalization to other bases is only a minor change and not something students seem to need to learn from scratch? Why is this viewed as a formula and not the most basic definition?</a></p> <h2 id="column-perspective">Column Perspective</h2> <p>In case you’ve seen me rant online about how column perspective is the ultimate definition of matrix vector multiplication, perhaps now you can see why I think so. With our perspective of a matrix as a compact definition for a linear transformation, this definition is just obvious.</p> \[\begin{equation} \Bigg(\begin{matrix}a_1&amp;\cdots&amp;a_n\end{matrix}\Bigg) \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} =x_1a_1+\ldots+x_na_n \end{equation}\] <p>where \(a_i\) is the \(i\)th column of \(A\) (which, by definition, is \(Ae_i\)). As a side note, the following two facts are things that most students I’ve worked with don’t find obvious, but really are with this perspective:</p> <ul> <li>The \(i\)th column of \(A\) has the preimage \(e_i\) (i.e. is equal to \(Ae_i\)). Again, literally <em>by definition</em>.</li> <li>If \(A\) is square and the \(i\)th column of \(A\) is a scalar multiple of \(e_i\), then \(e_i\) is an eigenvector.</li> </ul> <p>Observe, also, that taking some linear combination of the columns is the same as just taking the image of the vector with the scalars as the entries. That is, \(c_1a_1+c_2a_2+\ldots+c_na_n=A\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\). Hence, if you can find some combination of the column that gives you zero, then that gives you a vector in the kernel.</p> \[c_1a_1+\ldots+c_na_n=0\implies \begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\in\ker(A)\] <p>More generally, if you can find a combination of the columns to give you some vector \(b=c_1a_1+c_2a_2+\ldots+c_na_n\), then \(\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\) is a preimage of \(b\) under \(A\) (and thus a solution to \(Ax=b\)).</p> <p>So, then how does matrix-matrix multiplication fit into this perspective? Well, if \(T_A\) has the matrix \(A\), and \(T_B\) has the matrix \(B\) (and \(T_B\circ T_A\) is defined), then we want \(BAx=T_B(T_A(x))\). Once again, we just need to make sure that both \(BA\) and \(T_B\circ T_A\) map the standard basis vectors to the same thing.</p> \[T_B(T_A(e_j))=T_B(a_j),\quad BAe_j=Ba_j\] <p>And with our definition, there’s nothing immediately wrong or strange we have to do. We just define the \(j\)th column of \(BA\) to be \(Ba_j\). That is,</p> \[\begin{multline} BA=B\Bigg(\begin{matrix}a_1&amp;\ldots&amp;a_n\end{matrix}\Bigg)\\ =\Bigg(\begin{matrix}Ba_1&amp;\ldots&amp;Ba_n\end{matrix}\Bigg) \end{multline}\] <p>If you think about it, this is actually quite reasonable. \(Ax\) is going to be some linear combination of the columns of \(A\). And \(B(Ax)\) is just applying \(B\) to every single one of those vectors in the linear combination. So the image of the standard basis vectors under \(BA\) is just \(B\) applied to the images of the standard basis vectors under \(A\).</p> <p>Am I going to talk about the row column rule now? No. Because there are a million videos on youtube about it, and your professor probably drilled it into your head. This post is focused on <em>intuition</em>. It doesn’t take that long to see that the row column rule follows directly from the column definition, once you just combine it all into one vector. To be clear, the row column rule is not evil, and I definitely use it because it’s quicker. But I think it’s better to keep in mind the column definition is really <em>the</em> ultimate definition, and it ties pretty much <em>everything</em> you do with matrices together.</p> <p><img src="/assets/img/all coming together.png" alt="yes i'm using it again" style="width:100%; max-width:600px;"/></p> <h3 id="a-remark-on-column-space">A remark on column space</h3> <p>Alright, so at the beginning I talked about how we call the set of all outputs of a function the “image” or “range”. For some reason, in linear algebra, we try to obscure the connection between the image/range of a linear transformation and the image/range of a matrix function by calling it a different name: “the column space of \(A\)”. This is an intuitive name for the span/set of all linear combinations of the columns of \(A\), but it distracts from the fact that we already define multiplication by \(A\) to be <em>taking linear combinations of the columns of \(A\)</em>.</p> <p>This is one of the most frustrating parts of tutoring linear algebra for me. Because for most students, span, column space, and range are basically separate concepts in their head, when it’s really just <em>one</em> thing. The fact that</p> <blockquote> <p>”\(Ax=b\) is consistent if and only if \(b\) is in the column space of \(A\)”</p> </blockquote> <p>has to be a <em>theorem</em> and is not as self-evident as “\(f(x)=y\) is possible if and only if \(y\) is an output of \(f\)” makes me want to tear my hair out. And it’s not the student’s fault! It’s how they’re being taught linear algebra.</p> <p>So screw “column space” (and “null space” for that matter). All matrix functions in linear algebra are linear transformations, and we’re interested in the image and kernel of them. Consider this yeeting two redundant terms from the subject entirely.</p> <p>(Plus, you’ll always sound smarter if you say “image” and “kernel” instead of “column space” and “null space”)</p> <h2 id="column-relationships">Column relationships</h2> <p>Alright, let’s talk about a new matrix \(A=\begin{pmatrix}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{pmatrix}\). This is a \(3\times4\) matrix, so it’s a transformation from \(F^4\to F^3\). Now, as is usual in linear algebra, we’re interested in the image and kernel of this linear transformation.</p> <p>The matrix is wider than it is tall, and so it’s trying to stuff a higher dimensional space into a lower dimensional one. Thus, it’s intuitive that we’ll have a nontrivial kernel. A wider matrix <em>can</em> map to the entire codomain, but it’s not immediately obvious if every vector in \(F^3\) <em>will</em> actually have a preimage under \(A\). It’s not even immediately obvious how you’d even determine that! All we theoretically know is that an output is a linear combination of the columns. So, can we reach any \(F^3\) vector with a linear combination of these columns? If not, how can we tell?</p> <p>Well, if we think about it, what we really <em>want</em> is a basis for the range of \(A\). Of course, every column is in the range, so we just want to know which columns to take for a basis, and which columns are redundant. If we look carefully, we can see one major relationship: Column 2 = -(Column 1). This means that if we use both columns one and two, then our representations won’t be unique (because they are linearly dependent). So we should definitely exclude column 2 if we’re going to use column 1.</p> <p>If we look at Column 3, it doesn’t have any obvious relationship to Column 1. In fact, you can show they are linearly independent. But, you might just notice that Column 4 = Column 1 + Column 3. Another dependence relationship. Thus, it appears that we also have to exclude Column 4, leaving just Columns 1 and 3. Hence,</p> \[\left\{\begin{pmatrix}1\\1\\1\end{pmatrix},\begin{pmatrix}1\\2\\3\end{pmatrix}\right\}\] <p>is a basis for the image of \(A\). You can confirm this is true, because we can uniquely write every column of \(A\) as a linear combination of these vectors, so then we can write any linear combination of the columns as a linear combination of these vectors. And from here, we can see that the image is two-dimensional, so it won’t span or generate \(F^3\). Thus, \(A\) is not a surjective or onto transformation.</p> <p>But what of the kernel? Well, Column 2 = -(Column 1) is actually just saying</p> \[A(e_2)=-A(e_1)\implies A(e_1+e_2)=0\] <p>So \(\begin{pmatrix}1\\1\\0\\0\end{pmatrix}\) is in the kernel. Can you see how by similar logic \(\begin{pmatrix}1\\0\\1\\-1\end{pmatrix}\) is also in the kernel? You can verify it yourself by applying \(A\) to both vectors, and when you use column perspective, it should become clear. And we can justify through rank-nullity that these bases are complete because their sizes sum to the dimension of the domain (but let’s just vibe it).</p> <p>Okay, so I lead you through a problem you would have learned to do by row reduction without it. Any liquored-up hillbilly with a shotgun could have done that at the zoo. (TODO: remove this)</p> <p>Let’s try a much easier, <em>seemingly unrelated</em> problem:</p> \[R=\begin{pmatrix}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{pmatrix}\] <p>Here, it’s so much easier to find a basis for the image and kernel. Every column is a linear combination of the independent vectors \(e_1,e_2\), so they form a basis for the image. Oh, and look! Column 1 is \(e_1\) and Column 3 is \(e_2\). So Column 1 and Column 3 themselves form a basis for the image.</p> <p>As for the kernel, clearly if we take Column 1 + Column 2, we’ll cancel them out and get zero. So actually similar to above \(\begin{pmatrix}1\\1\\0\\0\end{pmatrix}\) is in the kernel. And, similarly, it’s pretty easy to see that if we take Column 4 and then subtract off Columns 1 and 3, then that’s also zero. So \(\begin{pmatrix}1\\0\\1\\-1\end{pmatrix}\) is also in the kernel. Now… that seems familiar. Both \(A\) and \(R\) have the same kernel, the same column relationships, and the same columns form a basis for the image*. What is this sorcery??</p> <p>Note*: It turns out that the kernel actually uniquely determines which columns can be a basis for the image.</p> <h3 id="cr-decomposition">CR Decomposition</h3> <p>It turns out that \(A\) and \(R\) are actually significantly linked. Because \(R\) can be obtained from \(A\) by applying an invertible operator on the left. But there is also a more fundamental observation we can make. We established that each column of \(A\) can be uniquely written as a linear combination of \(\left\{\begin{pmatrix}1\\1\\1\end{pmatrix},\begin{pmatrix}1\\2\\3\end{pmatrix}\right\}\). Well, let’s look at the linear combination for each column. We’ll call the vectors in that set \(v_1\) and \(v_2\) respectively.</p> \[A=\begin{pmatrix}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{pmatrix}\] <p>Well, the columns of \(A\) are, respectively: \(1v_1\), \(-1v_1\), \(1v_2\), \(1v_1+1v_2\). Thus, you can verify that</p> \[A=\begin{pmatrix}1&amp;1\\1&amp;2\\1&amp;3\end{pmatrix} \begin{pmatrix}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\end{pmatrix}\] <p>We often call this \(CR\) decomposition, since \(C\) contains a basis for the columns of \(A\), and \(R\) contains a basis for the rows of \(A\). Notice how the columns of the right matrix essentially tell you what linear combination of the columns of \(C\) you need to get the corresponding column of \(A\). We call these the “coordinate vectors” with respect to the pivot column basis.</p> <h2 id="row-reduction">Row reduction</h2> <p>Disregarding the \(CR\) decomposition, let’s talk about what I meant by “\(R\) can be obtained from \(A\) by applying an invertible operator on the left”.</p> \[\begin{multline*} \begin{pmatrix} 0 &amp; 3 &amp; -2 \\ 0 &amp; -1 &amp; 1 \\ 1 &amp; -2 &amp; 1 \end{pmatrix}\begin{pmatrix}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{pmatrix}\\ =\begin{pmatrix}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{pmatrix} \end{multline*}\] <p>And since it’s invertible, we can also obtain \(A\) from \(R\) by applying the inverse on the left.</p> <p>Note: If you want to know how I found the matrix, you can just augment \(A\) with the identity and row reduce. You will get \(R\) augmented with a matrix that puts \(A\) into its RREF.</p> <p>So the question becomes “why does applying an invertible operator on the left not change the kernel or column relationships”? Well, like I said above, it turns out that the kernel actually uniquely determines the column relationships of the matrix. See <a href="#Kernel determines RREF">the end of the post</a> for an algorithm that gives you the unique set of nonzero RREF rows that has the specified kernel.</p> <p>So then why does applying an invertible operator on the left preserve the kernel? Well, this is actually a lot easier. If \(E\) is invertible, then</p> \[Ax=0\iff EAx=0\] <p>So if \(EA=R\), then \(Ax=0\iff Rx=0\implies \ker(A)=\ker(R)\).</p> <p>So, in summary, invertible operations on the left of a matrix don’t change the kernel, which also tell us which choices of columns give us a basis for the image.</p> <h3 id="row-space">Row space</h3> <p>There is another perspective for what an “invertible operator on the left” actually means using <a href="../columnperspective/#row-perspective" target="_blank">“row perspective”</a>. It’s essentially the transpose of column perspective.</p> \[\begin{equation} AB=\begin{pmatrix} {A}_1^T\\{A}_2^T\\\vdots\\{A}_m^T \end{pmatrix}{B}= \begin{pmatrix} {A}_1^T{B}\\{A}_2^T{B}\\\vdots\\{A}_m^T{B} \end{pmatrix} \end{equation}\] <p>Where we are denoting the \(i\)th row of \(A\) as \(A_i^T\).</p> <p>If we think about what \(x^TB\) means, it’s just the transpose of \(B^Tx\), which is taking a linear combination of the columns of \(B^T\), which are the rows of \(B\). Then \(x^TB\) is just a row vector which is a linear combination of the rows of \(B\). Thus, the rows of \(AB\) are just linear combinations of the rows of \(B\). This means that every row of \(AB\) is in the row space of \(B\), and if \(A\) is <em>invertible</em>, then the row space of \(B\) is going to be entirely preserved!</p> <p>This is where elementary matrices come into play. It’s a nifty theorem that every invertible matrix is some product of elementary matrices. That is, any invertible matrix is equivalent to some sequence of row operations. In a way, elementary matrices “generate” the invertible matrices (but not in exactly the same way a set of vectors “generates” or spans a subspace. Here it’s through noncommutative products and not linear combinations).</p> <p>So when we say an “invertible operator on the left”, that can be thought of as performing some elementary row operations. Which, through all we have thus far established, preserves the row space, and thus also the kernel, and thus also the column relationships.</p> <p>In case you forgot, the elementary row operations are the following. I encourage you to convince yourself that these three operations would preserve column relationships.</p> <ol> <li>Swapping rows</li> <li>Scaling a row by a nonzero amount</li> <li>Adding a multiple of one row to another</li> </ol> <p>So, in summary, this is why we do row operations. They preserve the most fundamental aspects of a matrix (besides the image itself): the kernel and which columns generate the image. They can therefore be used to “simplify” a matrix. To its reduced row echelon form, for instance. The RREF, as we have seen, being the clearest and simplest way to see the relationship between the columns of the matrix.</p> <h4 id="row-space-basis">Row space basis</h4> <p>Another quick remark: We’ve established that row reduction preserves the row space. And, looking at the structure of the RREF, it’s clear that the nonzero rows of the RREF are linearly independent. They thus form a basis for the row space. Note also that since we can switch around rows when row reducing, that we must take the RREF rows as our row space basis rather than the corresponding rows of the original matrix. I explain that more in-depth <a href="../rowcolspace/#row-space" target="_blank">here</a>. But I also want to point out the difference in how the RREF gives us a basis for the row space and column space of the original matrix.</p> <p>Since row reduction can swap and change rows, we usually don’t end up with rows from the original matrix, but we do end up with a much nicer basis. Columns, on the other hand, never change their relative position, and that’s one of the reasons the relationships are preserved. However, though the rows of the RREF are still in the row space (just linear combinations of the original), the resulting columns are almost always completely different. This is why we go back to the original matrix. But this means that our column space basis is usually not as nice.</p> <p>In summary: the RREF of \(A\) gives a nicer row space basis not usually containing the rows of \(A\), while it gives a not so nice column space basis using the original columns of \(A\). This means that the RREF of \(A^T\) gives a nicer column space basis for \(A\) not in terms of the original columns, but a not so nice basis for the row space of \(A\) in terms of the original rows.</p> <p>All this to say, that row reduction not only shows us the relationships between the columns, but it also gives us the nicest possible basis for the rows. That means, we can also use the RREF to get the nicest possible basis for the span of any arbitrary set of vectors by sticking the vectors as the rows of a matrix and row reducing. Functionally also giving us a way to determine dimension computationally: just count the number of nonzero rows of the RREF.</p> <h3 id="solving-systems-of-equations">Solving systems of equations</h3> <p>Now, you may realize that I’m <em>only</em> getting to <em>solving</em> systems of equations <em>now</em>. And this may be surprising because row operations and matrices are often introduced FOR solving systems of equations. But hear me out! Everything we have talked about thus far is going to make row reduction’s application to solving systems of equations extremely simple and intuitive.</p> <p>See, we have thus far established row operations and row reduction as a way to see and make clear the relationship between a matrix’s columns. That is, row reduction can allow us to more easily see how to write one column as a linear combination of the others. Let’s see how this applies to systems of equations.</p> <p>A system of equations has approximately four “forms”. The one we use most commonly is \(Ax=b\). Each perspective has its own intuitive benefit. \(Ax=b\), for example, emphasizes that we’re trying to find a preimage under the linear transformation defined by \(A\). For example,</p> \[\begin{pmatrix}1&amp;-1&amp;1\\1&amp;-1&amp;2\\1&amp;-1&amp;3\end{pmatrix}x=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <p>But we can also write it in</p> <ul> <li>Equation form: Emphasizes that we are trying to simultaneously satisfy multiple constraints.</li> </ul> \[\begin{matrix}1x_1&amp;-&amp;1x_2&amp;+&amp;1x_3&amp;=&amp;2\\1x_1&amp;-&amp;1x_2&amp;+&amp;2x_3&amp;=&amp;3\\1x_1&amp;-&amp;1x_2&amp;+&amp;3x_3&amp;=&amp;4\end{matrix}\] <ul> <li>Vector form: Column perspective, basically. Emphasizes we are trying to write one vector as a linear combination of some starting set of vectors (the columns of \(A\))</li> </ul> \[x_1\begin{pmatrix}1\\1\\1\end{pmatrix}+x_2\begin{pmatrix}1\\1\\1\end{pmatrix} +x_3\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <ul> <li>Augmented matrix form: The most compact form of the system, and is going to be our secret weapon using the view we’ve built up in this post.</li> </ul> \[\left(\begin{array}{ccc\|c}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{array}\right)\] <p>You may have noticed that this augmented matrix is the matrix \(A\) from the <a href="#column-relationships">Column relationships</a> section! Recall that we found that Column 4 = Column 1 + Column 3. That is,</p> \[\begin{pmatrix}1\\1\\1\end{pmatrix} +\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <p>And, again, we can easily see this is true by looking at the RREF:</p> \[\left(\begin{array}{ccc\|c}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{array}\right) \sim \left(\begin{array}{ccc\|c}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{array}\right)\] <p>This may seem completely inconsistent with how you learned it. You probably learned to just row reduce and then turn it back into equation form. Which would be</p> \[\begin{matrix}1x_1&amp;-&amp;1x_2&amp;&amp;&amp;=&amp;1\\&amp;&amp;&amp;&amp;x_3&amp;=&amp;1\end{matrix}\] <p>And then solve for the pivot variables blah blah blah. But, instead, you could just look at the columns.</p> <p>The augmented column 4, or \(b\) column, is Column 1 + Column 3. So a <strong>particular solution</strong> is \(\begin{pmatrix}1\\0\\1\end{pmatrix}\). We can identify that the kernel vector, or <strong>homogeneous solution</strong>, associated with the nonpivot column (Column 2) of the coefficient matrix is \(\begin{pmatrix}1\\1\\0\end{pmatrix}\) because Column 1 + Column 2 = 0. And that gives us our <strong>general solution</strong>: a particular solution plus a linear combination of the homogeneous solutions (a basis of the kernel of the coefficient matrix).</p> \[x=\begin{pmatrix}1\\0\\1\end{pmatrix}+c\begin{pmatrix}1\\1\\0\end{pmatrix}\] <p>I’m not saying this is the one only good way to solve systems, but I’m trying to convey the fact that with this perspective of matrices, matrix multiplication, and row reduction I’ve set up, the application to solving system of equations is clear and intuitive, and it can be explained in a way that all of these concepts can be tied together.</p> <p><img src="/assets/img/all coming together.png" alt="yes i'm using it again again" style="width:100%; max-width:600px;"/></p> <h3 id="kernel-determines-rref">Kernel determines RREF</h3> <p>Alright, now I didn’t want to stick this in the middle of the post. I had a sort of rage flow going. But this is an algorithm that allows you to determine the RREF directly from the kernel. Specifically, from the kernel alone you can actually determine exactly what the nonzero rows of rref(\(A\)) must be. Here’s how:</p> <ol> <li>Put your basis for \(\ker(A)\) as the rows of a matrix</li> <li>row reduce and find the kernel of that new matrix</li> <li>take the basis of that new kernel as the rows of <em>another</em> matrix and row reduce</li> <li>the result will be the nonzero rows of rref(\(A\)).</li> </ol> <p>The algorithm is mostly just something for you to think about. It’s a little advanced to explain exactly why it works, but it has to do with the fact that the row space is the orthogonal complement of the kernel, and the nonzero rows of the RREF are just a specific form of basis for the row space. I encourage you to try to think it through.</p> <p><a href="https://youtu.be/NtiGP0MlF8k?si=T5evvei871Yl76GT" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><category term="best"/><summary type="html"><![CDATA[What does the RREF tell us, and why do we spend so much time on it? Why do we define matrix multiplication the way we do?]]></summary></entry><entry><title type="html">Jordan Canonical Form Made Easier Part 1</title><link href="http://localhost:8080/blog/2024/jordan/" rel="alternate" type="text/html" title="Jordan Canonical Form Made Easier Part 1"/><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>http://localhost:8080/blog/2024/jordan</id><content type="html" xml:base="http://localhost:8080/blog/2024/jordan/"><![CDATA[<p>Okay, so Jordan Canonical Form is a very strange topic. It’s usually relegated to advanced linear algebra (which it is), but it’s also presented in a very strange way. You see, JCF is actually part of a much deeper result found in Abstract Algebra! Specifically: Ring Theory about finitely generated modules over a PID (principal ideal domain). That’s a lot of words!</p> <p>The idea is that finitely generated Modules over a PID have a very predictable and clear structure. And we can actually identify every linear operator on some vector space \(V\) (over the field \(F\)) with a finitely generated torsion module \(V\) over the ring \(F[x]\) (all polynomials with coefficients in \(F\) with variable \(x\)). And after a lot of incredibly difficult abstract algebra, you can prove the existence of a Jordan Canonical form!</p> <p>Is the abstract algebra actually better than the seemingly random theorems and proofs used to prove the existence of JCF without abstract algebra? Well… I think so. But the point I <em>really</em> want to get across to you is the following:</p> <ol> <li>Proving the existence of the Jordan Canonical Form is HARD and unintuitive</li> <li>Using the JCF to find a Jordan basis isn’t nearly as bad</li> </ol> <p>So my plan is to offload the existence of the Jordan form to abstract algebra so that we can focus on using our knowledge of <a href="../changeofbasis/" target="_blank">change of basis</a> to actually <em>find</em> the Jordan form and Jordan basis. And then in a second follow up part, I will show some ways to make the existence more intuitive.</p> <h2 id="direct-sum-of-eigenspaces">Direct Sum of Eigenspaces</h2> <p>So we’re assuming that our transformation \(T\) / matrix \(A\) can be represented in <em>some</em> basis as a Jordan matrix. This also means we’re assuming that the characteristic polynomial splits over our field, so we might be over \(\mathbb{C}\). So we are assuming that \(A\) can be written as a block diagonal matrix of Jordan blocks:</p> \[[A]_\beta=\begin{pmatrix}J_1\\&amp;J_2\\&amp;&amp;\ddots\\&amp;&amp;&amp;J_k\end{pmatrix}\] <p>If you didn’t know, we can write this as a “direct sum of matrices”</p> \[\begin{pmatrix}J_1\\&amp;J_2\\&amp;&amp;\ddots\\&amp;&amp;&amp;J_k\end{pmatrix} =J_1\oplus J_2\oplus\ldots\oplus J_k\] <p>where each \(J_i\) is a Jordan block corresponding to some eigenvalue \(\lambda\)</p> \[J_i=\begin{pmatrix}\lambda&amp;1&amp;0&amp;\cdots&amp;0&amp;0\\ 0&amp;\lambda&amp;1&amp;\cdots&amp;0&amp;0\\ \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ 0&amp;0&amp;0&amp;\cdots&amp;\lambda&amp;1\\ 0&amp;0&amp;0&amp;\cdots&amp;0&amp;\lambda\end{pmatrix} =\lambda I_n+N_{n}\] <p>where</p> \[N_{n}=\begin{pmatrix}0&amp;1&amp;0&amp;\cdots&amp;0&amp;0\\ 0&amp;0&amp;1&amp;\cdots&amp;0&amp;0\\ \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ 0&amp;0&amp;0&amp;\cdots&amp;0&amp;1\\ 0&amp;0&amp;0&amp;\cdots&amp;0&amp;0\end{pmatrix}\] <p>\(N\) is what I call the “canonical nilpotent matrix of order \(n\)”. Using \(\delta\) notation, \(N_{ij}=\delta_{i(j+1)}\).</p> <p>But here’s the cool thing about being a direct sum of matrices: it means each block is “independent” or “invariant” in a special way. Just like a vacation to Las Vegas, what happens in a Jordan Block stays in a Jordan Block. This means we can consider each Jordan block individually!</p> <p>Having to constantly write out all the possible cases for what the JCF of a transformation looks like is not just confusing for you, but also exhausting to LaTeX for me (which is much more important, I’m sure you can understand)!</p> <p>Notice how above, I didn’t label the diagonal of \(J_i\) with \(\lambda_i\). Because one eigenvalue can have multiple Jordan blocks! It just gets incredibly confusing. So we’re going to focus first on a single Jordan block and then talk about what happens when a single eigenvalue has multiple Jordan blocks.</p> <h2 id="jordan-blocks">Jordan Blocks</h2> <p>We are also going to talk, without loss of generality, about \(\lambda=0\). Literally nothing would change since we could define \(U=T-\lambda I\) and it would have the same Jordan basis. You will see that \(\lambda=0\) is also just much easier in general. In that case, that means we’re just talking about when the matrix for the transformation restricted to the subspace is \(N_n\) (nicely nilpotent).</p> <p>The columns of \(N\) are \(0,e_1,e_2,\ldots,e_{n-1}\). This means that if our Jordan basis for the subspace is \(\beta=\{v_1,\ldots,v_n\}\), then we have that</p> \[\begin{equation}\label{basis} Tv_1=0,\quad Tv_2=v_1,\quad \ldots\quad Tv_n=v_{n-1} \end{equation}\] <p>(If this isn’t clear to you, please review <a href="../changeofbasis/" target="_blank">change of basis</a>!)</p> <p>So, how do we find these vectors? Well, \(v_1\) seems easy enough. It’s just in the kernel of \(T\). However, there’s one more important requirement on \(v_1\): it must be in the image! Specifically, we need to have some preimage \(v_2\) of \(v_1\) under \(T\). And \(v_2\) needs to have a preimage \(v_3\) as well!</p> <p>This adds some complexity, as we need to make sure each vector has a preimage. How can we even do that? It turns out, we actually don’t have to! However, if our Jordan block is only size 2 or 3, then it’s actually usually not that hard to make sure you pick an eigenvector in the image. I’ll give two examples at the end of the post. Consider them optional exercises.</p> <h3 id="so-i-dont-have-to-do-that">So I don’t have to do that??</h3> <p>Nope! If you read \eqref{basis} from left to right, then you do have to ensure a preimage exists for each choice you make. But what if you go the other way? Well, we can notice that \(T^kv_j=v_{j-k}\) (where \(v_m=0\) if \(m\leq0\)). Which means that \(T^kv_k=0\). Thus, we can just start at the <em>end</em> of a Jordan chain and work backwards.</p> <p>So, first, we find a basis of the kernel for the highest power of \(T\) that isn’t zero (in this case \(n\)). Then, each of those vectors might generate a Jordan chain. i.e. we start at \(v_n\), and then we get \(v_{n-1}\) from \(Tv_n\). And \(v_{k}=T^{n-k}v_n\).</p> <p>If you want a more systematic way,</p> <h3 id="ensuring-the-eigenvector-is-in-the-image-examples">Ensuring the eigenvector is in the image examples</h3> \[A= \begin{pmatrix}-1&amp;1&amp;0\\-1&amp;1&amp;0\\-1&amp;1&amp;0\end{pmatrix}\] <p>This matrix has a trace and determinant of zero, so we know that \(0\) is an eigenvalue. In fact, we can observe it’s a rank one matrix, so knowing the trace is zero means all the eigenvalues are zero. If we row reduce \(A\) to find the eigenspace of zero, we get</p> \[\sim\begin{pmatrix}1&amp;-1&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;0\end{pmatrix}\] <p>which gives us two eigenvectors \(\begin{pmatrix}1\\1\\0\end{pmatrix}\) and \(\begin{pmatrix}0\\0\\1\end{pmatrix}\) by inspection (taking one of the third column would definitely give us zero. And adding the first two columns together would as well. See <a href="../columnperspective/" target="_blank">column perspective</a>).</p> <p>But neither of these eigenvectors are in the image! So what do we do? Well, in this case, it’s painfully obvious that a basis for the image is \(\begin{pmatrix}1\\1\\1\end{pmatrix}\). And, in fact, adding our eigenvectors together does give us this. This means that the first eigenvector of our eigenbasis can’t be either of the ones we originally computed. We actually had to pick a specific <em>combination</em> of them to ensure it’s in the image!</p> <p>Here’s another example.</p> \[A= \begin{pmatrix}-1&amp;1&amp;0\\-2&amp;1&amp;1\\-1&amp;1&amp;0\end{pmatrix}\] <p>Similarly, this matrix has a trace and determinant of zero, so we know that \(0\) is an eigenvalue. But it’s not a rank one matrix, so we have to do a bit more work. If we row reduce \(A\) to find the eigenspace of zero, we get</p> \[\sim\begin{pmatrix}1&amp;0&amp;-1\\0&amp;1&amp;-1\\0&amp;0&amp;0\end{pmatrix}\] <p>which gives us the eigenvector \(\begin{pmatrix}1\\1\\1\end{pmatrix}\) by inspection using similar logic as above.</p> <p>And, wow, our eigenvector is actually just the second column! In this case it was actually <em>easier</em>. If the kernel has dimension one, usually it’s actually easier because whether or not that kernel vector is in the image is actually equivalent to if the zero eigenspace is defective. However, it’s not always so obvious! If you want a <em>bit</em> of a challenge, try to find the preimage of the eigenvector of</p> \[\begin{pmatrix}2&amp;2&amp;-2\\5&amp;1&amp;-3\\1&amp;5&amp;-3\end{pmatrix}\] \[\begin{equation} \begin{pmatrix}1\\1\end{pmatrix} \end{equation}\] <p><a href="../eigentricks/" target="_blank">hyperlink</a></p> <p><a href="#c2">Section c2</a></p> <p><a href="https://youtu.be/g2VYkc-MtC8?si=7KvPWXyo4wPEhZ7f" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[not easy. just like... easier...]]></summary></entry><entry><title type="html">Introduction to Least Squares Part 2 (Electric Boogaloo)</title><link href="http://localhost:8080/blog/2024/leastsquarespart2/" rel="alternate" type="text/html" title="Introduction to Least Squares Part 2 (Electric Boogaloo)"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>http://localhost:8080/blog/2024/leastsquarespart2</id><content type="html" xml:base="http://localhost:8080/blog/2024/leastsquarespart2/"><![CDATA[<p>This is a short sequel to my previous post on <a href="../leastsquares/" target="_blank">least squares</a>.</p> <p>These are the questions I left at the end of that post (in the more general complex form).</p> <ol> <li>Why is \(\mathbf{A}^*\mathbf{A}\mathbf{x}=\mathbf{A}^*\mathbf{b}\) guaranteed a solution?</li> <li>Why would the solution to the normal equation actually be the “closest” solution?</li> </ol> <p>When I say “dot product” here, I mean the general euclidean inner product on \(\mathbb{C}^n\), where we multiply by the <em>conjugate</em> transpose (adjoint) rather than the regular transpose.</p> \[\langle w,v\rangle=v^*w=\left(\overline{v}\right)^Tw\] <p>Note that if we talk about \(\mathbb{R}^n\), then the adjoint is just, of course, the regular transpose.</p> <p>If you need a reminder for why we need the <em>conjugate</em>, it’s because complex numbers get weird when you just square them:</p> \[\begin{pmatrix}1&amp;i\end{pmatrix}\begin{pmatrix}1\\i\end{pmatrix}=1^2+i^2=0\] <p>In \(\mathbb{R}^n\), we say \(\left\lVert v \right\rVert^2=v^Tv\), but that would mean this nonzero vector has a magnitude of \(0\). This is a big no-no, as we should <em>only</em> have that the zero vector has a magnitude of \(0\).</p> <p>So we can’t just say \(w\cdot v=v^Tw\) and call it a day if we want to be as general as possible. But you are more than welcome to just interpret \(v^*\) as the transpose, and assume we’re talking in terms of real vectors (my complex compulsion prevents me from doing so).</p> <h2 id="why-is-it-consistent">Why is it consistent</h2> <p>Recall that a system of equations \({A}{x}={b}\) is consistent when \(b\) is in the image or column space of \(A\). So the column space, \(W\), is going to be the main character in our story today.</p> <p>We know that every vector space can be decomposed as the direct sum of any subspace and its orthogonal complement. That is,</p> \[\mathbb{C}^n=W\oplus W^\perp\] <p>This means that \(b\) can be written <em>uniquely</em> as \(b=w+w_\perp\), where \(w\) is \(W\) and \(w_\perp\) is orthogonal to everything in \(W\). Conceptually this can be understood as writing \(b\) as a sum of a vector which does make a consistent system, and a vector which can knock us out of being consistent (i.e. the system is consistent if and only if \(w_\perp=0\)).</p> <p>Remember that to be orthogonal in \(\mathbb{C}^n\), we need \(w\cdot v=v^*w=0\). This definition <em>does</em> mean the complex dot product is not commutative, but it is conjugate commutative (\(v\cdot w=\overline{w\cdot v}\)). So, since we are only interested in orthogonality and dot products of zero, we still get a sort of orthogonal symmetry/commutativity (\(v\cdot w=0\iff w\cdot v=0\)).</p> <p>This does add one complication. While in \(\mathbb{R}^n\), \(An=0\) implies that \(n\) is orthogonal to the rows of \(A\), that isn’t quite so in \(\mathbb{C}^n\), since the inner product requires one vector to be conjugated. That is, while in \(\mathbb{R}^n\), the row space is the orthogonal complement of the null space, in \(\mathbb{C}^n\), the null space is the orthogonal complement of the conjugated row space (\(\ker(B)^\perp=\operatorname{row}(\overline{B})\), which is also \(\operatorname{col}(B^*)\)). Note that if \(B\) is real, we do recover the result that the kernel is orthogonal to the row space.</p> <p>But if we let \(B=A^*\) in the expression above, this means the orthogonal complement of the null space of \(A^*\) is the column space of \(A\). That is, if a vector \(u\) is orthogonal to the column space of \(A\), then \(A^*u=0\).</p> <p>Thus, \(A^*w_\perp=0\) and so \(A^*b=A^*(w+w_\perp)=A^*w\). Essentially, this means that multiplying by the adjoint <em>sort of</em> projects us into the column space of \(A\) (or, at least, zeros out the part that isn’t in the column space).</p> <p>Since \(w\) is by definition in the column space of \(A\), we know \(Ax=w\) is consistent. Let \(x=c\) be the solution (that is, \(w=Ac\)).</p> <p>So let’s write out what happens when we multiply \(Ax=b\) by \(A^*\).</p> \[A^*Ax=A^*b=A^*w=A^*Ac\] <p>This clearly has a solution: \(x=c\), which actually implies that the solution to \(A^*Ax=A^*b\) is the same as the solution to \(Ax=w\).</p> <p>That is, the least squares solution is the solution to the system projected onto the column space. Which actually makes some sense intuitively. If we want the closest solution, we want it to be the solution to the system where the vector is projected orthogonally to the column space. Which actually answers both our questions.</p> <h2 id="why-is-it-the-closest-solution">Why is it the closest solution</h2> <p>Now, I said it answers both of our questions, but perhaps you aren’t quite convinced that just because it’s the solution to the system where \(b\) has been projected into the column space it actually minimizes our error. Let’s see if I can change your mind.</p> <p>We measure the “closest” solution using \(\left\lVert b-Ax \right\rVert^2\) (minimizing the squares of error: hence, ‘least squares’).</p> <p>Note that by orthogonality, we can say that \(\left\lVert w+w_\perp \right\rVert^2=\left\lVert w \right\rVert^2+\left\lVert w_\perp \right\rVert^2\). But, since \(w\) and \(Ax\) are both in \(W\), we can rewrite</p> \[\left\lVert b-Ax \right\rVert^2=\left\lVert w+w_\perp-Ax \right\rVert^2=\left\lVert w-Ax \right\rVert^2+\left\lVert w_\perp \right\rVert^2\] <p>Notice that no matter <em>what</em> \(x\) is, our squared error will always be \(\geq\left\lVert w_\perp \right\rVert^2\). This is hopefully somewhat intuitive. Since \(w_\perp\) is the part of \(b\) that makes our system inconsistent, its size acts as a lower bound for our error, telling us in a sense <em>how</em> inconsistent our system is. Cool, right?</p> <p>Thus, the only thing we <em>can</em> do to minimize error, is to minimize \(\left\lVert w-Ax \right\rVert^2\). The best we can do is make it zero. But… like we said before, \(w=Ac\) for some \(c\). Then, \(x=c\) will give us zero. So \(x=c\), the solution to \(Ax=w\), really <em>is</em> the solution that minimizes the error!</p> <p>One might object: what if \(A\) has dependent columns and there are multiple solutions to \(Ax=w\)? Well, we can see that <em>all</em> of those solutions will minimize the error. That is, even if the least squares solution is not unique, every single one will minimize the error! This is because the error is in terms of the magnitude of \(w-Ax\). And \(w-Ax=0\) if and only if \(x\) is a preimage of \(w\) of \(A\) (there is no restriction on <em>which preimage</em>).</p> <h2 id="the-smallest-closest-solution">The smallest closest solution</h2> <p>If you’re very picky, and you want to pick <em>just one</em> least squares solution, then you <em>could</em> theoretically pick the least squares solution with the minimum magnitude. If \(c\) is one least squares solution, then any other least squares solution will be of the form \(c+u\) where \(u\in\ker(A)\). Then, we can play a similar game in trying to minimize</p> \[\left\lVert c+u \right\rVert^2=\left\lVert k+k_\perp+u \right\rVert^2=\left\lVert k+u \right\rVert^2+\left\lVert k_\perp \right\rVert^2\] <p>where we are decomposing \(c\) in terms of \(k\in\ker(A)\) and \(k_\perp\in\ker(A)^\perp\). Since our degree of freedom is in choosing \(u\), we should pick \(u=-k\), and then we find that the ‘least squares least squares solution’ is the solution in the orthogonal complement of \(\ker(A)\).</p> \[x_{least}=\operatorname{proj}_{\ker(A)^\perp}(c)\] <p>If you are hankering for an expression for the projector into \(\ker(A)^\perp\), then one method one could use (especially if they’re in some sort of computing/programming environment) is to use the \(QR\) decomposition of \(A^*=QR\), and then</p> \[x_{least}=QQ^*c\] <p>This is because, as we said above, the orthogonal complement of \(\ker(A)\) is the column space of \(A^*\), and \(QQ^*\) from the \(QR\) decomposition projects orthogonally into the column space.</p> <p>We can actually do much better using the reduced SVD of \(A=USV^*\), where \(S\) is square and invertible. Then, you can show that</p> \[x_{least}=VS^{-1}U^*b\] <p>Which is why we call \(VS^{-1}U^*\) the “pseudo-inverse of \(A\)”. It always gives the smallest least squares solution! This is absolutely as good as it gets for a singular matrix.</p> <p>I think it’s a good exercise to try and show that \(VS^{-1}U^*b\) is in fact the smallest least squares solution! If you need some hints to get started,</p> <ul> <li>\(UU^*\) is a projector onto the column space of \(A\)</li> <li>\(VV^*\) is a projector onto the column space of \(A^*\), which is orthogonal to \(\ker(A)\)</li> <li>If \(b=w+w_\perp\), then what is \(w\) in terms of \(b\) and \(U\)?</li> </ul> <p><a href="https://youtu.be/M5CeQG1YfEQ?si=2J5M9Tdyq01GVAsc" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[Why the heck do we multiply by the transpose]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="http://localhost:8080/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>http://localhost:8080/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="http://localhost:8080/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="http://localhost:8080/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>http://localhost:8080/blog/2024/tabs</id><content type="html" xml:base="http://localhost:8080/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="aa5fdac1-3a54-4d1f-b6a8-83db55e473a9" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="aa5fdac1-3a54-4d1f-b6a8-83db55e473a9" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="68df3168-47e6-407d-ac11-748c2cf6e3f2" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="68df3168-47e6-407d-ac11-748c2cf6e3f2" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="23b62820-9491-4809-826a-3496552d7f14" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="23b62820-9491-4809-826a-3496552d7f14" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="http://localhost:8080/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>http://localhost:8080/blog/2024/typograms</id><content type="html" xml:base="http://localhost:8080/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="http://localhost:8080/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>http://localhost:8080/blog/2024/post-citation</id><content type="html" xml:base="http://localhost:8080/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="http://localhost:8080/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>http://localhost:8080/blog/2024/pseudocode</id><content type="html" xml:base="http://localhost:8080/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">The Wonderful World of Projectors</title><link href="http://localhost:8080/blog/2024/projectors/" rel="alternate" type="text/html" title="The Wonderful World of Projectors"/><published>2024-03-23T00:00:00+00:00</published><updated>2024-03-23T00:00:00+00:00</updated><id>http://localhost:8080/blog/2024/projectors</id><content type="html" xml:base="http://localhost:8080/blog/2024/projectors/"><![CDATA[<p>The following is my final presentation for my numerical linear algebra class (Spring 2023), the first graduate math course I ever took! I was really proud of it, and I feel like there’s not enough good resources about projectors, which I think are really really cool.</p> <p>For simplicity, we will let \(\mathbb{F}\) be \(\mathbb{R}\) or \(\mathbb{C}\).</p> <h1 id="projectors">Projectors</h1> <h2 id="idempotent-matrices">Idempotent Matrices</h2> <p><strong>Definition:</strong> A matrix \(P \in \mathbb{F}^{n \times n}\) is said to be a <strong>Projector</strong> if \(P^2 = P\). This matrix is also said to be ‘idempotent’. We say that \(P\) projects vectors <em>onto</em> \(\text{im}(P)\) <em>along</em> \(\text{ker}(P)\).</p> <p>Note that the only invertible projector is the identity matrix (this is a simple thing to prove).</p> <p>Why would this be the <em>definition</em> for a projector? What does \(P^2 = P\) have to do with projections? The general idea is this: Say \(\mathbb{F}^{n}=W\oplus K\), and we have \(P\) projects onto \(W\) along \(K\). That means that every vector \(x\) can be written in the form \(x=w+k\) for some \(w\in W\) and \(k\in K\), this representation is unique, and we expect that \(Px=w\) and \(Pw=w\), implying that \(Px=Pw\). This is because \(w\) is already in \(W\), so it shouldn’t change under the projection. If we examine this more closely, it means that</p> \[P^2x=P(Px)=Pw=Px\] \[\implies P^2x=Px\] <p>for all \(x\). Therefore, a projector should satisfy \(P^2=P\).</p> <p>We remark that this construction of \(Pw=w\) and \(Pk=0\) implies that we can decompose \(\mathbb{F}^n\) into a direct sum of the eigenspaces with eigenvalues \(1\) and \(0\). We will prove this shortly.</p> <p>To summarize: \(P\) takes in \(x\) and outputs the component of \(x\) in the direction of \(W\) (or the component in \(W\)). Thus, repeatedly applying \(P\) shouldn’t change the output because the result is already <em>in</em> \(W\).</p> <p>It’s not necessarily obvious that if \(P^2=P\) then \(P\) is a projector, however. To prove that, we need some more groundwork. Some questions we need to answer first: How do we know that \(\mathbb{F}^n\) will be a direct sum of the image and kernel of \(P\)?</p> <p>From this point on, we assume that \(P\) satisfies \(P^2=P\).</p> <p><strong>Proposition:</strong> \(\text{im}(P) \oplus \text{ker}(P) = \mathbb{F}^n\)</p> <p><strong>Proof:</strong> For all \(x \in \mathbb{F}^n\),</p> \[x = Px + (x - Px)\] \[P(x - Px) = Px - P^2x = Px - Px = 0\] \[\implies x - Px \in \text{ker}(P)\] <p>Therefore, all \(x\) can be written as the sum of some \(Px \in \text{im}(P)\) and \(x - Px \in \text{ker}(P)\). Thus, \(\mathbb{F}^n = \text{im}(P) + \text{ker}(P)\). If</p> \[\begin{multline*} v \in \text{im}(P) \cap \text{ker}(P) \\\implies v = Px \land Pv = 0 = P(Px) = Px = v \\\implies v = 0 \end{multline*}\] <p>Therefore,</p> \[\mathbb{F}^n = \text{im}(P) \oplus \text{ker}(P)\] <h2 id="eigenspaces-of-projectors">Eigenspaces of Projectors</h2> <p>Since all but the trivial projector (\(I\)) are not invertible, then they usually have a nontrivial kernel, which is the eigenspace for zero.</p> <p><strong>Proposition:</strong> The only eigenvalues for a projector \(P\) are \(\lambda=0,1\).</p> <p><strong>Proof:</strong> Suppose that \(\lambda\) is an eigenvalue of \(P\) with eigenvector \(v \neq 0\). Then</p> \[Pv=\lambda v \implies P^2v=Pv=\lambda v=\lambda Pv=\lambda^2v\] \[\implies (\lambda^2-\lambda) v=0 \implies \lambda=0,1\] <p><strong>Proposition:</strong> For a projector \(P\), the eigenspace for \(\lambda=1\), \(E_1=\text{im}(P)\).</p> <p><strong>Proof:</strong> Suppose \(v \in \text{im}(P)\). Then there exists an \(x \in \mathbb{R}^n\) such that</p> \[v=Px \implies Pv=P^2x=Px=v\] <p>Then \(Pv=v\). If \(v \in \text{ker}(P)\), then \(Pv=0=v\). Thus, either \(v\) is an eigenvector with eigenvalue \(1\), or \(v=0\). That is, \(\text{im}(P) \subseteq E_1\).</p> <p>The other direction is quite simple. If \(v\) is an eigenvector with eigenvalue \(1\), then \(Pv=v\) implies \(v \in \text{im}(P)\). Therefore,</p> \[\text{im}(P)=E_1\] <p><strong>Proposition:</strong> All projectors are diagonalizable.</p> <p><strong>Proof:</strong> Since \(\text{im}(P)=E_1\) and \(\text{ker}(P)=E_0\), by Proposition, we can say that</p> \[\mathbb{F}^n=\text{im}(P) \oplus \text{ker}(P)=E_1 \oplus E_0\] <p>Since \(\mathbb{F}^n\) is a direct sum of eigenspaces of \(P\), then \(P\) must be diagonalizable (as this implies there exists a basis of eigenvectors).</p> <p>For a slightly slicker and less intuitive proof (something I definitely love), we can also use that</p> \[P^2=P \implies P^2-P=P(P-I)=0\] <p>A rather obscure linear algebra factoid: \(A\) is diagonalizable if and only if there is some polynomial \(f(x)\) that is a product of distinct linear factors such that \(f(A)=0\). This is basically saying that a matrix is diagonalizable matrix if its minimal polynomial is a product of distinct linear factors (we only want Jordan blocks of size 1).</p> <p>Hence, if \(f(x)=x(x-1)\), which is a product of distinct linear factors, then \(f(P)=0\). Since such a polynomial exists, that is enough to prove \(P\) is diagonalizable.</p> <h2 id="complementary-projectors">Complementary Projectors</h2> <p><strong>Theorem:</strong> Suppose \(P\) is the projector onto the subspace \(W \leq \mathbb{F}^n\), and let \(K=\ker(P)\). We previously proved that \(\mathbb{F}^n=W \oplus K\), so for all vectors \(v \in \mathbb{F}^n\), \(v\) can be written uniquely as a linear combination</p> \[v=w+k\] <p>where \(w \in W\) and \(k \in K\).</p> <p>How can we find \(w\) and \(k\)? To find \(w\), we can use the projector \(P\) for \(W\). But what about \(k\)? To find it, we can do</p> \[k=v-w=v-Pv=(I-P)v\] <p>We can verify as well that \(I-P\) is indeed also a projector.</p> \[\begin{multline*} (I-P)^2=I^2-2P+P^2\\=I-2P+P=I-P \end{multline*}\] <p>Therefore, we define the complementary projector of \(P\) as follows:</p> <p><strong>Definition:</strong> The <strong>complementary projector</strong> of the projector \(P\) is \(I-P\), which projects onto the kernel of \(P\).</p> <p>So if \(P\) is a projector, it projects onto its image along its kernel. While \(I-P\) is also a projector, which projects onto the kernel of \(P\) along the image of \(P\). It’s a good exercise to convince yourself of this. If \(Pv=\lambda v\), then what is \((I-P)v\)? Then see what happens if \(\lambda=0,1\).</p> <p>If you look back at the proof for Proposition, you’ll notice that we essentially wrote</p> \[x=Px+(I-P)x\] <p>which is writing \(x\) as the sum of its component in \(\text{im}(P)\) and its component in \(\text{im}(I-P)=\ker(P)\).</p> <h2 id="orthogonal-projectors">Orthogonal Projectors</h2> <p><strong>Definition:</strong> A projector \(P\) is an <strong>orthogonal projector</strong> if \(\ker(P)^\perp=\text{im}(P)\). That is, if the subspace it is projecting onto is orthogonal to the subspace it is projecting along. If \(P\) is not an orthogonal projector, then it is an <strong>oblique</strong> projector.</p> <p><strong>Proposition:</strong> A projector \(P\) is an orthogonal projector if and only if \(P\) is hermitian.</p> <p><strong>Proof:</strong> Suppose \(P \in \mathbb{F}^{n\times n}\) is an orthogonal projection matrix. That is, \(\text{im}(P)\) is the orthogonal complement to \(\ker(P)\). Suppose \(\beta_i=\{v_1,\ldots,v_r\}\) is an orthonormal basis for the image (\(E_1\)), and \(\beta_k=\{u_1,\ldots,u_k\}\) is an orthonormal basis for the kernel (\(E_0\)). As we proved previously, \(\beta=\beta_i \cup \beta_k\) is an eigenbasis for \(\mathbb{F}^n\). But, since they are orthogonal complements, \(\beta\) is an orthogonal basis. Since \(P\) has an orthonormal eigenbasis, and it has purely real eigenvalues, then \(P\) must be hermitian.</p> <p>If \(P\) is a hermitian projector, then its eigenspaces are orthogonal, because it is hermitian. Therefore, \(E_0^\perp=\ker(P)^\perp=E_1=\text{im}(P)\). Hence, \(\ker(P)^\perp=\text{im}(P)\). Therefore, \(P\) is an orthogonal projector.</p> <p>Since \(\ker(P)\) is the orthogonal complement to \(\text{im}(P)\), then \(I-P\) is specifically the projection onto the orthogonal complement of \(\text{im}(P)\). Thus, we get this next nice little corollary</p> <p><strong>Corollary:</strong> If \(P\) is the orthogonal projector onto a subspace \(W\), then \(I-P\) is the orthogonal projector onto \(W^\perp\).</p> <h2 id="constructing-projector-matrices">Constructing Projector Matrices</h2> <h3 id="diagonalization">Diagonalization</h3> <p>There are two ways to do this. If you have a basis for the image (suppose it is \(r\) dimensional) and kernel (making sure the union is a basis for \(\mathbb{F}^n\)), then put the vectors as columns in a matrix \(M\), and</p> \[P=M\begin{pmatrix}I_r &amp; 0 \\ 0 &amp; 0\end{pmatrix}M^{-1}\] <p>Which is… unwieldy at best. Especially since you actually need a basis for the kernel. But looking at it <em>this</em> way does make it a little more clear why \(P\) is a projector. This essentially tells us we can get a basis for \(\mathbb{F}^n\) which are either vectors in \(E_1\) or in \(E_0\). If we are in \(E_1\), then \(P\) doesn’t change it (which is how a projection should act), and if we are anywhere else but \(E_1\), then we are in \(E_0\), so \(P\) takes it to zero (also the way we would expect a projector to act).</p> <p>This form also makes it a little more clear why \(I-P\) is a projector. Since</p> \[I-P=M\begin{pmatrix}0 &amp; 0 \\ 0 &amp; I_{n-r}\end{pmatrix}M^{-1}\] <p>This is also pretty much one of the only ways to construct an oblique projector.</p> <p>But this is generally not how we generally compute projectors.</p> <h3 id="rank-one-oblique-projectors">Rank one oblique projectors</h3> <p>A special case of this is when the subspace we are projecting along is one-dimensional. So suppose that \(w\) is a basis for a one-dimensional subspace \(W\), \(v\) is a basis for the orthogonal complement of another \(n-1\) dimensional subspace \(K\) such that \(\mathbb{F}^n=W \oplus K\), and \(v^*w \neq 0\). Then</p> \[P=\frac{wv^*}{v^*w}\] <p>is the projector onto \(W\) along \(K\). You can also just start with any \(v\) not orthogonal to \(w\), and then the above matrix automatically projects onto \(W\) along \(\text{span}\{v\}^\perp\).</p> <p>For example,</p> \[P=\begin{pmatrix}1 \\ 1\end{pmatrix}\begin{pmatrix}3 &amp; -2\end{pmatrix}=\begin{pmatrix}3 &amp; -2 \\ 3 &amp; -2\end{pmatrix}\] <p>is the oblique projector onto \(\text{span}\left\{\begin{pmatrix}1 \\ 1\end{pmatrix}\right\}\) along \(\text{span}\left\{\begin{pmatrix}2 \\ 3\end{pmatrix}\right\}\).</p> <h3 id="column-space-orthogonal-projectors">Column space orthogonal projectors</h3> <p>The other way to construct a projector matrix is to use a basis for the image. Let’s say we want \(P\) to be a projector on a subspace \(W\), and we have a basis \(\{v_1, \ldots, v_r\}\) and we put them as the columns of a matrix \(A\). Note that \(A\) will have full column rank, so \(A^*A\) will be invertible. Then</p> \[P=A(A^*A)^{-1}A^*\] <p><strong>Proof:</strong> For all \(x \in \mathbb{F}^n\), we can write</p> \[x=x_W+x_{W^\perp}\] <p>uniquely, where \(x_W \in W\) and \(x_{W^\perp} \in W^\perp\). Our goal is to find some projector \(P\) such that \(Px=x_W\). <br/> Since the columns of \(A\) form a basis for \(W\), then there exists some \(c \in \mathbb{F}^r\) (the coordinate vector with respect to the basis) such that \(x_W=Ac\). Multiplying \(x=x_W+x_{W^\perp}=Ac+x_{W^\perp}\) on both sides by \(A^*\), we obtain</p> \[A^*x=A^*Ac+A^*x_{W^\perp}=A^*Ac+0\] <p>Since the entries of \(A^*x_{W^\perp}\) will be the inner product of vectors in \(W\) with a vector in \(W^\perp\), then that term zeros out. Because \(A\) is full rank, then \(A^*A\) is invertible so,</p> \[c=(A^*A)^{-1}A^*x\] \[\implies A(A^*A)^{-1}A^*x = Ac = x_W\] <p>Therefore, \(P=A(A^*A)^{-1}A^*\) is the projector.</p> <p>Now, while I think it makes it a little less intuitive why this should be a projector, this is definitely <em>much</em> easier to compute. And it makes the fact that it satisfies \(P^2=P\) pretty clear. Verify it!</p> <p>The only thing to note is that this always produces an orthogonal projector (since we can see this matrix is Hermitian by inspection). But that is actually a good thing because orthogonal projectors are actually much better than oblique projectors.</p> <p>Now, I think you’d agree that the \(A^*A\) inverse is pretty annoying. How can we get rid of it? Well, the easiest way would be to ensure that \(A^*A=I\). Then the projector is just \(AA^*\). So, how can we construct a matrix such that \(A^{-1}=A^*\)? The answer is: use an orthonormal basis.</p> <h1 id="orthonormal-bases">Orthonormal Bases</h1> <p><strong>Definition:</strong> A set of vectors is <strong>orthogonal</strong> if all of the vectors have inner product 0.</p> <p>Note that an orthogonal set of vectors may not be linearly independent, because a set can have the zero vector and still be orthogonal. This is because the zero vector is orthogonal to all vectors. However, we <em>can</em> say it is linearly independent if all the vectors are nonzero. We won’t prove this for an orthogonal set, but only for orthonormal sets. The proof is almost identical, however.</p> <p>For the sake of simplicity, though, when we say a set is “orthogonal” we will assume all the vectors are nonzero.</p> <p><strong>Definition:</strong> A set is <strong>orthonormal</strong> if it is an orthogonal set and all of the vectors are unit vectors.</p> <p><strong>Remark:</strong> One way we notate an orthonormal basis is as follows: Suppose \(\{u_1, \ldots, u_n\}\) is orthonormal, then</p> \[\langle u_i, u_j \rangle = u_j^*u_i = \begin{cases} 1, &amp; i=j \\ 0, &amp; i \neq j \end{cases}\] <p><strong>Theorem:</strong> An orthonormal set is linearly independent.</p> <p><strong>Proof:</strong> Suppose \(\{v_1, \ldots, v_n\}\) is an orthonormal set, and</p> \[c_1v_1 + \ldots + c_nv_n = 0\] <p>Dot both sides with \(v_i\), and all terms will cancel except for \(v_i \cdot c_iv_i\).</p> \[c_i(v_i \cdot v_i) = c_i||v_i||^2 = c_i = 0\] <p>Therefore, the vectors are linearly independent.</p> <h2 id="why-are-orthonormal-bases-so-great">Why are orthonormal bases so great?</h2> <p>Let’s list some reasons:</p> <ol> <li>We don’t have to check linear independence to verify it’s a valid basis. Only that we have the right number of unit vectors, and they are orthogonal. Orthogonality is much easier to check than linear independence or span.</li> <li>If a square matrix has orthonormal columns, then the inverse is as almost as easy as it gets. It’s just the adjoint! Even if it isn’t square, it will be easy to cancel.</li> <li>It is <em>extremely</em> easy to find the coefficients of a linear combination. We don’t have to solve a system of equations.</li> <li>It’s really easy to project onto a subspace when you have an orthonormal basis for it.</li> <li>Creating a projection matrix is <em>so much easier</em> if you have an orthonormal basis.</li> <li>You can always make a basis orthonormal.</li> </ol> <p>It is for these reasons that real symmetric/hermitian matrices are simply the best matrices, objectively. Because they are guaranteed to have an orthonormal eigenbasis. And we all know that an eigenbasis is the best basis, so an orthonormal eigenbasis is as good as it gets.</p> <p>The other classes of normal matrices (matrices which are unitarily diagonalizable) are also good for similar reasons (Unitary matrices are also fantastic), but <em>their</em> eigenvalues aren’t guaranteed to be real.</p> <h3 id="finding-coefficients-of-a-linear-combination">Finding coefficients of a linear combination</h3> <p>Imagine we have some vector \(x \in \mathbb{F}^n\), and we want to find the coefficients for a linear combination of an orthonormal basis \(B=\{v_1, \ldots, v_n\}\). Then we want</p> \[x = c_1v_1 + \ldots + c_nv_n\] <p>As we did before, we can dot both sides by \(v_i\) to obtain just \(c_i\) on the right-hand side.</p> \[v_i \cdot x = c_i\] <p>And there we go. The coefficients are obtained simply from the dot product. No system of equations required.</p> <p><strong>Theorem:</strong> If \(\{v_1, \ldots, v_n\}\) is an orthonormal basis for \(\mathbb{F}^n\), then for all \(x \in \mathbb{F}^n\)</p> \[x = (v_1 \cdot x)v_1 + \ldots + (v_n \cdot x)v_n\] <p>But notice that \(v_i \cdot x = v_i^*x\). If we let \(U = \left( v_1 \quad \cdots \quad v_n \right)\), then we can obtain the vector of coefficients from</p> \[\begin{pmatrix} v_1^*x \\ \vdots \\ v_n^*x \end{pmatrix} = U^*x\] <p><strong>Remark:</strong> It follows that</p> \[\begin{multline*} x = (v_1 \cdot x)v_1 + \ldots + (v_n \cdot x)v_n \\= U(U^*x) = UU^*x \end{multline*}\] <p>implying that \(UU^* = I\).</p> <h3 id="example">Example</h3> <p>Take the orthonormal basis for \(\mathbb{R}^2\)</p> \[\beta = \left\{ \frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix}, \frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix} \right\}\] <p>and say we want to express the vector \(\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix}\) as a linear combination of those vectors.</p> \[\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} =c_1 \frac{\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix}}{\sqrt{2\pi^2+e^2}} + c_2\frac{\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\] <p>I do not care what you say, you simply <em>cannot</em> make me solve that system of equations. But, luckily, we don’t have to! The dot products are given by the transpose of the matrix</p> \[\frac{\begin{pmatrix} \pi\sqrt{2} &amp; e \\ -e &amp; \pi\sqrt{2} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} = \frac{\begin{pmatrix} \pi\sqrt{10} + e\pi^2 \\ \pi^3\sqrt{2} - e\sqrt{5} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\] <p>Therefore,</p> \[\begin{multline*} \begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} \\= \frac{\pi\sqrt{10} + e\pi^2}{2\pi^2 + e^2}\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix} + \frac{\pi^3\sqrt{2} - e\sqrt{5}}{2\pi^2 + e^2}\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix} \end{multline*}\] <p>As a bonus, we get the inverse of that nasty matrix to be</p> \[\begin{multline*} \left(\frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix}\pi\sqrt{2}&amp;-e\\e&amp;\pi\sqrt{2}\end{pmatrix}\right)^{-1}\\ =\frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix}\pi\sqrt{2}&amp;e\\-e&amp;\pi\sqrt{2}\end{pmatrix} \end{multline*}\] <h2 id="unitary-matrices">Unitary matrices</h2> <p>The matrix \(A^*A\) is Hermitian, and every entry is just the inner product of the columns. Thus, if the columns are orthonormal, then everything off the diagonal will be zero, and every diagonal entry will be the norm squared, which is one. Thus, \(A^*A=I\) if and only if the columns of \(A\) form an orthonormal basis. This means \(A^{-1}=A^*\) if \(A\) is square. But, even if it isn’t, \(A^*A\) will still yield an identity matrix, allowing us to cancel both sides.</p> <h3 id="fun-facts-about-unitary-matrices">Fun facts about unitary matrices</h3> <p>If \(U\) is a square unitary matrix, then</p> <ol> <li>\(U\) is unitarily diagonalizable</li> <li>All of the eigenvalues of \(U\) are of the form \(\lambda_j=e^{i\theta_j}\). That is, \(\vert \lambda_j\vert=1\).</li> <li>There exists a hermitian matrix \(H\) such that \(U=e^{iH}\). If \(U=VDV^*\), and \(\lambda_j=e^{i\theta_j}\), then one such matrix is \(H=V\text{diag}\{\theta_1, \ldots, \theta_n\}V^*\).</li> <li>\(\langle x, y \rangle = \langle Ux, Uy \rangle\). That is, \(U\) preserves the angle between vectors.</li> <li>\(\vert\vert x\vert\vert = \vert\vert Ux\vert\vert\). That is, \(U\) preserves the length of all vectors.</li> </ol> <p>Just know, this only scratches the surface. Unitary matrices are simply astounding.</p> <h2 id="orthogonal-projections-when-you-have-an-orthonormal-basis">Orthogonal Projections when you have an orthonormal basis</h2> <p><strong>Theorem:</strong> If the columns of \(Q\) form an orthonormal basis for a subspace \(W\), then</p> \[P=QQ^*\] <p>is the orthogonal projector onto \(W\).</p> <p>If \(\{v_1, \ldots, v_r\}\) is an orthonormal basis for a subspace \(W\), then we can simply obtain the projection as</p> \[\text{proj}_W(x) = (v_1 \cdot x)v_1 + \ldots + (v_r \cdot x)v_r\] <p>Since no matter what the other vectors in the orthogonal complement to \(W\) happen to be (which, if we completed the orthonormal basis, the rest of the vectors would have to be a basis for \(W^\perp\)), they would dot to zero. Therefore, if \(Q = \left( v_1 \quad \cdots \quad v_r \right)\), then the projection matrix is simply</p> \[P = QQ^*\] <p>No \(A^*A\) inverse shenanigans. The reason is that \(Q^*Q = I\), so the formula \(Q(Q^*Q)^{-1} Q^*\) just reduces to \(QQ^*\).</p> <p><strong>Remark:</strong> The \(Q\) in</p> \[P = QQ^*\] <p>can actually be a \(Q\) from the \(QR\) factorization of any matrix \(A\) with full column rank and the same column space. Since \(R\) will be invertible,</p> \[\begin{multline*} A(A^*A)^{-1} A^* \\ = QR((QR)^*QR)^{-1} (QR)^* \\ = QR(R^*Q^*QR)^{-1}(R^*Q^*)\\ = QR(R^*R)^{-1} R^*Q^* \\ = QR(R^{-1} (R^*)^{-1})R^*Q^* \\ = QQ^* \end{multline*}\] <p><strong>Remark:</strong> If \(A\) has orthogonal columns instead of orthonormal, then \(A^*A \neq I\), but it will be diagonal. So \(A(A^*A)^{-1} A^*\) is not that bad. However, the diagonal entries are just the norm squared, so putting \((A^*A)^{-1}\) is equivalent to dividing the columns of \(A\) (and the rows of \(A^*\)) by the norm, normalizing the columns. Thus, we still get the same answer.</p> <p><a href="https://youtu.be/T2kOj-GFN8k?si=So_pJbTwG_n3-BWH" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor Fisher</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[They're cool, I promise!]]></summary></entry></feed>