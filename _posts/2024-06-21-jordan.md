---
layout: distill
title: Jordan Canonical Form Made Easier Part 1
date: 2024-6-21
description: not easy. just like... easier...
comments: true
importance: 3
tags:
category: linear-algebra
authors:  
  - name: Taylor F.
    url: ""
    affiliations:
      name: None
toc:
  - name: Direct Sum of Eigenspaces
  - name: Jordan Blocks
  - name: Ensuring the eigenvector is in the image examples
---

Okay, so Jordan Canonical Form is a very strange topic. It's usually relegated to advanced linear algebra (which it is), but it's also presented in a very strange way. You see, JCF is actually part of a much deeper result found in Abstract Algebra! Specifically: Ring Theory about finitely generated modules over a PID (principal ideal domain). That's a lot of words!

The idea is that finitely generated Modules over a PID have a very predictable and clear structure. And we can actually identify every linear operator on some vector space $$V$$ (over the field $$F$$) with a finitely generated torsion module $$V$$ over the ring $$F[x]$$ (all polynomials with coefficients in $$F$$ with variable $$x$$). And after a lot of incredibly difficult abstract algebra, you can prove the existence of a Jordan Canonical form! 

Is the abstract algebra actually better than the seemingly random theorems and proofs used to prove the existence of JCF without abstract algebra? Well... I think so. But the point I *really* want to get across to you is the following:

1. Proving the existence of the Jordan Canonical Form is HARD and unintuitive
2. Using the JCF to find a Jordan basis isn't nearly as bad

So my plan is to offload the existence of the Jordan form to abstract algebra so that we can focus on using our knowledge of [change of basis](../changeofbasis/){:target="_blank"} to actually *find* the Jordan form and Jordan basis. And then in a second follow up part, I will show some ways to make the existence more intuitive.

## Direct Sum of Eigenspaces

So we're assuming that our transformation $$T$$ / matrix $$A$$ can be represented in *some* basis as a Jordan matrix. This also means we're assuming that the characteristic polynomial splits over our field, so we might be over $$\mathbb{C}$$. So we are assuming that $$A$$ can be written as a block diagonal matrix of Jordan blocks:

$$[A]_\beta=\begin{pmatrix}J_1\\&J_2\\&&\ddots\\&&&J_k\end{pmatrix}$$

If you didn't know, we can write this as a "direct sum of matrices"

$$\begin{pmatrix}J_1\\&J_2\\&&\ddots\\&&&J_k\end{pmatrix}
=J_1\oplus J_2\oplus\ldots\oplus J_k$$

where each $$J_i$$ is a Jordan block corresponding to some eigenvalue $$\lambda$$

$$J_i=\begin{pmatrix}\lambda&1&0&\cdots&0&0\\
0&\lambda&1&\cdots&0&0\\
\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&\cdots&\lambda&1\\
0&0&0&\cdots&0&\lambda\end{pmatrix}
=\lambda I_n+N_{n}$$

where

$$N_{n}=\begin{pmatrix}0&1&0&\cdots&0&0\\
0&0&1&\cdots&0&0\\
\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&\cdots&0&1\\
0&0&0&\cdots&0&0\end{pmatrix}$$

$$N$$ is what I call the "canonical nilpotent matrix of order $$n$$". Using $$\delta$$ notation, $$N_{ij}=\delta_{i(j+1)}$$.

But here's the cool thing about being a direct sum of matrices: it means each block is "independent" or "invariant" in a special way. Just like a vacation to Las Vegas, what happens in a Jordan Block stays in a Jordan Block. This means we can consider each Jordan block individually!

Having to constantly write out all the possible cases for what the JCF of a transformation looks like is not just confusing for you, but also exhausting to LaTeX for me (which is much more important, I'm sure you can understand)! 

Notice how above, I didn't label the diagonal of $$J_i$$ with $$\lambda_i$$. Because one eigenvalue can have multiple Jordan blocks! It just gets incredibly confusing. So we're going to focus first on a single Jordan block and then talk about what happens when a single eigenvalue has multiple Jordan blocks.

## Jordan Blocks

We are also going to talk, without loss of generality, about $$\lambda=0$$. Literally nothing would change since we could define $$U=T-\lambda I$$ and it would have the same Jordan basis. You will see that $$\lambda=0$$ is also just much easier in general. In that case, that means we're just talking about when the matrix for the transformation restricted to the subspace is $$N_n$$ (nicely nilpotent).

The columns of $$N$$ are $$0,e_1,e_2,\ldots,e_{n-1}$$. This means that if our Jordan basis for the subspace is $$\beta=\{v_1,\ldots,v_n\}$$, then we have that

$$\begin{equation}\label{basis}
Tv_1=0,\quad Tv_2=v_1,\quad \ldots\quad Tv_n=v_{n-1}
\end{equation}$$

(If this isn't clear to you, please review [change of basis](../changeofbasis/){:target="_blank"}!)

So, how do we find these vectors? Well, $$v_1$$ seems easy enough. It's just in the kernel of $$T$$. However, there's one more important requirement on $$v_1$$: it must be in the image! Specifically, we need to have some preimage $$v_2$$ of $$v_1$$ under $$T$$. And $$v_2$$ needs to have a preimage $$v_3$$ as well!

This adds some complexity, as we need to make sure each vector has a preimage. How can we even do that? It turns out, we actually don't have to! However, if our Jordan block is only size 2 or 3, then it's actually usually not that hard to make sure you pick an eigenvector in the image. I'll give two examples at the end of the post. Consider them optional exercises.

### So I don't have to do that??

Nope! If you read \eqref{basis} from left to right, then you do have to ensure a preimage exists for each choice you make. But what if you go the other way? Well, we can notice that $$T^kv_j=v_{j-k}$$ (where $$v_m=0$$ if $$m\leq0$$). Which means that $$T^kv_k=0$$. Thus, we can just start at the *end* of a Jordan chain and work backwards.

So, first, we find a basis of the kernel for the highest power of $$T$$ that isn't zero (in this case $$n$$). Then, each of those vectors might generate a Jordan chain. i.e. we start at $$v_n$$, and then we get $$v_{n-1}$$ from $$Tv_n$$. And $$v_{k}=T^{n-k}v_n$$.

If you want a more systematic way,

### Ensuring the eigenvector is in the image examples

$$A=
\begin{pmatrix}-1&1&0\\-1&1&0\\-1&1&0\end{pmatrix}$$

This matrix has a trace and determinant of zero, so we know that $$0$$ is an eigenvalue. In fact, we can observe it's a rank one matrix, so knowing the trace is zero means all the eigenvalues are zero. If we row reduce $$A$$ to find the eigenspace of zero, we get

$$\sim\begin{pmatrix}1&-1&0\\0&0&0\\0&0&0\end{pmatrix}$$

which gives us two eigenvectors $$\begin{pmatrix}1\\1\\0\end{pmatrix}$$ and $$\begin{pmatrix}0\\0\\1\end{pmatrix}$$ by inspection (taking one of the third column would definitely give us zero. And adding the first two columns together would as well. See [column perspective](../columnperspective/){:target="_blank"}). 

But neither of these eigenvectors are in the image! So what do we do? Well, in this case, it's painfully obvious that a basis for the image is $$\begin{pmatrix}1\\1\\1\end{pmatrix}$$. And, in fact, adding our eigenvectors together does give us this. This means that the first eigenvector of our eigenbasis can't be either of the ones we originally computed. We actually had to pick a specific *combination* of them to ensure it's in the image!

Here's another example.

$$A=
\begin{pmatrix}-1&1&0\\-2&1&1\\-1&1&0\end{pmatrix}$$

Similarly, this matrix has a trace and determinant of zero, so we know that $$0$$ is an eigenvalue. But it's not a rank one matrix, so we have to do a bit more work. If we row reduce $$A$$ to find the eigenspace of zero, we get

$$\sim\begin{pmatrix}1&0&-1\\0&1&-1\\0&0&0\end{pmatrix}$$

which gives us the eigenvector $$\begin{pmatrix}1\\1\\1\end{pmatrix}$$ by inspection using similar logic as above.

And, wow, our eigenvector is actually just the second column! In this case it was actually *easier*. If the kernel has dimension one, usually it's actually easier because whether or not that kernel vector is in the image is actually equivalent to if the zero eigenspace is defective. However, it's not always so obvious! If you want a *bit* of a challenge, try to find the preimage of the eigenvector of

$$\begin{pmatrix}2&2&-2\\5&1&-3\\1&5&-3\end{pmatrix}$$

$$
\begin{equation}
\begin{pmatrix}1\\1\end{pmatrix}
\end{equation}
$$

[hyperlink](../eigentricks/){:target="_blank"}

[Section c2](#c2)


[hyperlink](https://youtu.be/g2VYkc-MtC8?si=7KvPWXyo4wPEhZ7f){:target="_blank"}
