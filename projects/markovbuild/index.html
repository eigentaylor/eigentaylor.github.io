<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Constructing 2x2 Markov Matrices | eigentaylor </title> <meta name="author" content="Taylor Eigen Fisher"> <meta name="description" content="Build a Markov Chain with desired behavior."> <meta name="keywords" content="math, blog, differential-equations, linear-algebra"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="http://localhost:8080/projects/markovbuild/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> eigentaylor </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Constructing 2x2 Markov Matrices</h1> <p class="post-description">Build a Markov Chain with desired behavior.</p> </header> <article> <h2 id="what-we-want">What We Want</h2> <p>We are looking for a regular stochastic matrix \(M\) with all positive entries less than \(1\) such that the columns add up to \(1\). The ‘steady-state’ vector will be \(\vec{x}_f=(x_f,1-x_f)\), where \(0&lt; x_f&lt;1\), such that</p> <ol> <li> \[\begin{equation} M\vec{x}_f=\vec{x}_f \end{equation}\] </li> <li> \[\begin{equation} \lim_{k\to\infty}M^k\vec{x}_0=\vec{x}_f,\;\forall \;\vec{x}_0 \end{equation}\] </li> </ol> <p>So, essentially, we have a specific vector we want the system to converge to. That isn’t too difficult so let’s spice it up and get more specific. After \(n\) iterations, for any given initial state \(\vec{x}_0=(x_0,1-x_0)\) where \(0\leq x_0\leq1\), the system must be within an error of \(\varepsilon&gt;0\) from the equilibrium solution.</p> \[\begin{equation} |M^n\vec{x}_0-\vec{x}_f|\leq\varepsilon \end{equation}\] <p>The solution which we will derive is as follows.</p> <hr> <p>For a \(\lambda\) satisfying both</p> \[\begin{equation}\label{bounds} | \lambda |\leq \sqrt[\leftroot{-2}\uproot{2}n]{\frac{\varepsilon}{x_f\sqrt2}},\quad \lambda&gt;\frac{|2x_f-1|-1}{|2x_f-1|+1} \end{equation}\] <hr> <h2 id="the-matrix">The Matrix</h2> <p>\(M\) is given by both of the following expressions,</p> <hr> \[\begin{equation} \label{M2Sum} M= \begin{bmatrix}x_f\\1-x_f\end{bmatrix} \begin{bmatrix}1&amp;1\end{bmatrix}+ \lambda\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \end{equation}\] \[\begin{equation}\label{M1Sum} M= I+(1-\lambda) \begin{bmatrix}1\\-1\end{bmatrix}\begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \end{equation}\] <hr> <p>The sign of \(\lambda\) will determine whether the system approaches the equilibrium directly or if it bounces around like an alternating series.</p> <p>Additionally, \(\vec{x}_n=M^n\vec{x}_0\) is given by</p> \[\begin{equation} \label{xn} M^n\vec{x}_0= \vec{x}_f + \lambda^n(x_f-x_0)\begin{bmatrix}-1\\1\end{bmatrix} \end{equation}\] <p>This looks exactly like what we want since</p> \[\lim_{n\to \infty}\lambda^n=0\] <p>if \(| \lambda |&lt;1\) leaving us with our steady state vector in the limit.</p> <h2 id="testing">Testing</h2> <p>Let’s take an example to see if it works.</p> <p>I want the equilibrium of the system to be \(\vec{x}_f=(0.75,0.25)\), and after \(4\) iterations I want the system to be within \(0.01\) from the equilibrium.</p> <p>That gives me</p> \[\begin{equation} | \lambda |\leq \sqrt[\leftroot{-2}\uproot{2}4]{\frac{0.01}{0.75\sqrt2}} \approx 0.3116,\quad \lambda &gt; -\frac{1}{3} \end{equation}\] <p>Let’s take \(\lambda=0.3\). That gives me</p> \[\begin{equation} M= I+(0.7) \begin{bmatrix}1\\-1\end{bmatrix}\begin{bmatrix}-0.25&amp;0.75\end{bmatrix}=\begin{bmatrix}0.825&amp;0.525\\0.175&amp;0.475\end{bmatrix} \end{equation}\] <p>We test our equilibrium vector,</p> \[\begin{equation} \begin{bmatrix}0.825&amp;0.525\\0.175&amp;0.475\end{bmatrix}\begin{bmatrix}0.75\\0.25\end{bmatrix}=\begin{bmatrix}0.75\\0.25\end{bmatrix} \end{equation}\] <p>Perfect. Now to maximize the error, \(x_0\) has to be either \(0\) or \(1\) (because based on equation \eqref{xn} we want to maximize \(| x_f-x_0 |\) where they are both between \(0\) and \(1\)). That means \(\vec{x}_0\) is either \((1,0)\) or \((0,1)\). Therefore, the \(\vec{x}_n\) furthest from the equilibrium vector will be one of the columns of \(M^n\). Thus, we can just raise \(M\) to the \(n\)th power and see how close each column is to \(\vec{x}_f\).</p> \[\begin{equation} M^4=\begin{bmatrix}0.752&amp;0.7439\\0.248&amp;0.2561\end{bmatrix} \end{equation}\] <p>The error of each column is \(\approx 0.00286,0.00859\) which are both absolutely within our desired error. Just to check, let’s see how far off \(\vec{x}_3\) is.</p> \[\begin{equation} M^3=\begin{bmatrix}0.7567&amp;0.7297\\0.2432&amp;0.2702\end{bmatrix} \end{equation}\] <p>The error here is \(\approx 0.0095,0.0286\), the latter of which it is not within our desired error. So we managed to set the rate of convergence to be just where we want it. In fact, we could get a lower bound on \(\lambda\) by substituting \(3\) into the upper bound expression in \eqref{bounds}. Since \(\lambda=-0.3\) also fits between our bounds, we could have used that to get a different matrix where the answer oscillates.</p> \[\begin{array}{c|ccccc} \lambda&amp;M^0&amp;M&amp;M^2&amp;M^3&amp;M^4\\ 0.3&amp;\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} &amp;\begin{bmatrix}0.825&amp;0.525\\0.175&amp;0.475\end{bmatrix} &amp;\begin{bmatrix}0.7725&amp;0.6825\\0.2275&amp;0.3175\end{bmatrix} &amp;\begin{bmatrix}0.7567&amp;0.7297\\0.2432&amp;0.2702\end{bmatrix} &amp;\begin{bmatrix}0.7520&amp;0.7439\\0.2480&amp;0.2561\end{bmatrix}&amp;\hline\\ -0.3&amp;\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} &amp;\begin{bmatrix}0.675&amp;0.975\\0.325&amp;0.025\end{bmatrix} &amp;\begin{bmatrix}0.7725&amp;0.6825\\0.2275&amp;0.3175\end{bmatrix} &amp;\begin{bmatrix}0.7433&amp;0.7703\\ 0.2567&amp; 0.2297\end{bmatrix} &amp;\begin{bmatrix}0.7520 &amp; 0.7439\\ 0.2480 &amp; 0.2561\end{bmatrix} \end{array}\] <p>Beautifully, they are the same for even \(n\), which is consistent with \eqref{xn}.</p> <h2 id="proof--derivation">Proof / Derivation</h2> <p>We start by using an incredibly useful fact:</p> <p>If \(v\) is an eigenvector of \(A\) with eigenvalue \(\lambda\), and if \(v'\) is an eigenvector of \(A^T\) with eigenvalue \(\lambda'\neq\lambda\), then</p> \[\begin{equation} v\cdot v'=0 \end{equation}\] <p>I encourage the reader to prove this.</p> <details> <summary>Hint</summary> Consider $$(v')^TAv$$ </details> <p>Now we also use the fact that \((1,1)\) is an eigenvector with eigenvalue \(1\) of \(M^T\) because the columns of \(M\) add up to \(1\).</p> <p>Therefore, \((-1,1)\) is an eigenvector of \(M\) with an eigenvalue that is <em>not</em> \(1\) (let’s say \(\lambda\)).</p> <p>Since we want \((x_f,1-x_f)\) (which is linearly independent of \((-1,1)\) if \(0&lt; x_f&lt;1\)) to be an eigenvector with eigenvalue \(1\), we have a basis of eigenvectors and everything we need to construct a diagonalization of \(M\).</p> \[\begin{equation} M=\begin{bmatrix}x_f&amp;-1\\1-x_f&amp;1\end{bmatrix} \begin{bmatrix}1&amp;0\\0&amp;\lambda\end{bmatrix}\begin{bmatrix}x_f&amp;-1\\1-x_f&amp;1\end{bmatrix}^{-1} \end{equation}\] <p>To get \eqref{M2Sum} we just write the center matrix as</p> \[\begin{equation} \begin{bmatrix}1&amp;0\\0&amp;\lambda\end{bmatrix}=e_1e_1^T+\lambda e_2e_2^T \end{equation}\] <p>Then we just distribute the matrices on the left and right.</p> <p>For \eqref{M1Sum}, we simplify calculating by removing an \(I\) from \(M\).</p> \[\begin{equation} M=I+\begin{bmatrix}x_f&amp;-1\\1-x_f&amp;1\end{bmatrix} \begin{bmatrix}0&amp;0\\0&amp;\lambda-1\end{bmatrix}\begin{bmatrix}x_f&amp;-1\\1-x_f&amp;1\end{bmatrix}^{-1} \end{equation}\] <p>Big brain row/column perspective tells us that we want \((\lambda-1)\) of the second column of the eigenvector matrix times the second row of its inverse.</p> \[\begin{equation} M=I+(\lambda-1)\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \end{equation}\] <p>\(\lambda-1\) is always negative, so just to make it nice we negate both it and the column vector to get \eqref{M1Sum}.</p> <p>To prove \eqref{xn} we use the wonderful properties of diagonalization \(M^n=PD^nP^{-1}\) and do the same steps that we did to get \eqref{M2Sum} to say that</p> \[\begin{equation} M^n= \begin{bmatrix}x_f\\1-x_f\end{bmatrix} \begin{bmatrix}1&amp;1\end{bmatrix}+ \lambda^n\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \end{equation}\] <p>Multiply on the right by \(\vec{x}_0=(x_0,1-x_0)\),</p> \[\begin{equation} M^n\vec{x}_0= \begin{bmatrix}x_f\\1-x_f\end{bmatrix} \begin{bmatrix}1&amp;1\end{bmatrix} \begin{bmatrix}x_0\\1-x_0\end{bmatrix}+ \lambda^n\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \begin{bmatrix}x_0\\1-x_0\end{bmatrix} \end{equation}\] \[\begin{equation} M^n\vec{x}_0= \begin{bmatrix}x_f\\1-x_f\end{bmatrix}+ \lambda^n(x_f-x_0)\begin{bmatrix}-1\\1\end{bmatrix} \end{equation}\] <p>Leaving us with \eqref{xn}.</p> <p>Now to get the bounds on \(\lambda\), we examine \(| M^n\vec{x}_0-\vec{x}_f | &lt;\varepsilon\),</p> \[\begin{gather} | M^n\vec{x}_0-\vec{x}_f | &lt;\varepsilon\\ \left| \vec{x}_f+\lambda^n(x_f-x_0)\begin{bmatrix}-1\\1\end{bmatrix}-\vec{x}_f \right| &lt;\varepsilon\\ \left| \lambda^n(x_f-x_0)\begin{bmatrix}-1\\1\end{bmatrix} \right| &lt;\varepsilon\\ | \lambda^n(x_f-x_0) |\left| \begin{bmatrix}-1\\1\end{bmatrix} \right| &lt;\varepsilon\\ | \lambda|^n |x_f-x_0| \sqrt2 &lt;\varepsilon\\ | \lambda|^n |x_f-x_0| &lt;\frac{\varepsilon}{\sqrt2}\\ \end{gather}\] <p>Now we observe that the maximum value of \(| x_f-x_0 |\) is \(x_f\) given our constraints on \(x_0\) and \(x_f\) to be between zero and one. Therefore, to maximize the distance, \(x_0\) would have to be either of the bounds, zero or one. Substituting \(| x_f-x_0 |=x_f\) and dividing it over,</p> \[\begin{equation} | \lambda|^n &lt;\frac{\varepsilon}{x_f\sqrt2} \end{equation}\] <p>By taking the \(n\)-th root, we get the first part (upper bound) of \eqref{bounds}.</p> <p>Now to get the lower bound, we make sure the entries of \(M\) satisfy \(0&lt; M_{ij}&lt;1\). We need only ensure one entry in each column, as \(0&lt; M_{ij}&lt;1\implies 0&lt; 1-M_{ij}&lt;1\). So we choose to look at \(M_{12}\) and \(M_{21}\) because it’ll be just slightly easier.</p> \[\begin{gather} M_{12}=e_1^TMe_2\\ 0&lt; e_1^TMe_2&lt;1\\ 0&lt; e_1^T\left( I+(\lambda-1)\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \right)e_2&lt;1\\ 0&lt; 0+(\lambda-1)(-1)(x_f)&lt;1\\ -\frac{1}{x_f}&lt; \lambda-1&lt;0\\ 1-\frac{1}{x_f}&lt; \lambda&lt;1\\ \left(1-\frac{1}{x_f}\right)^{1}&lt; \lambda&lt;1\\ \end{gather}\] <p>hhhh</p> \[\begin{gather} M_{21}=e_2^TMe_1\\ 0&lt; e_2^TMe_1&lt;1\\ 0&lt; e_2^T\left( I+(\lambda-1)\begin{bmatrix}-1\\1\end{bmatrix} \begin{bmatrix}x_f-1&amp;x_f\end{bmatrix} \right)e_1&lt;1\\ 0&lt; 0+(\lambda-1)(1)(x_f-1)&lt;1 \end{gather}\] <p>We know that \(x_f-1&lt; 0\) so we flip the inequality.</p> \[\begin{gather} -\frac{1}{1-x_f}&lt; \lambda-1&lt;0\\ 1-\frac{1}{1-x_f}&lt; \lambda&lt;1\\ \left(1-\frac{1}{x_f}\right)^{-1}&lt; \lambda&lt;1\\ \end{gather}\] <p>Therefore, for a lower bound on \(\lambda\),</p> \[\begin{equation} \left(1-\frac{1}{x_f}\right)^{\pm1}&lt; \lambda \end{equation}\] <p>We can simplify this, however. The following are equivalent expressions:</p> \[\begin{equation} \left(1-\frac{1}{x_f}\right)^{\pm1}&lt; \lambda \implies \frac{|2x_f-1|-1}{|2x_f-1|+1} &lt; \lambda \end{equation}\] <p>concluding the proof of \eqref{bounds}.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Taylor Eigen Fisher. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: June 24, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>